{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import os\n",
    "import netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\justi\\OneDrive\\Desktop\\projectes\\hardware-accelerators\n",
      "Files in current directory: ['.devcontainer', '.git', '.github', '.gitignore', '.vscode', 'data', 'hardware_accelerators', 'notebooks', 'pyproject.toml', 'README.md', 'reports', 'requirements.txt', 'resources.md', 'rtllib', 'tests']\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files in current directory:\", os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"data/mnist-12.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model IR version: 7\n",
      "Opset version: 12\n",
      "Producer name: CNTK\n",
      "\n",
      "Inputs:\n",
      "- Input3: 1\n",
      "  Shape: [1, 1, 28, 28]\n",
      "\n",
      "Outputs:\n",
      "- Plus214_Output_0: 1\n",
      "  Shape: [1, 10]\n"
     ]
    }
   ],
   "source": [
    "# Print basic model info\n",
    "print(f\"Model IR version: {model.ir_version}\")\n",
    "print(f\"Opset version: {model.opset_import[0].version}\")\n",
    "print(f\"Producer name: {model.producer_name}\")\n",
    "\n",
    "# Get graph\n",
    "graph = model.graph\n",
    "\n",
    "# Print input info\n",
    "print(\"\\nInputs:\")\n",
    "for input in graph.input:\n",
    "    print(f\"- {input.name}: {input.type.tensor_type.elem_type}\")\n",
    "    shape = [dim.dim_value for dim in input.type.tensor_type.shape.dim]\n",
    "    print(f\"  Shape: {shape}\")\n",
    "\n",
    "# Print output info\n",
    "print(\"\\nOutputs:\")\n",
    "for output in graph.output:\n",
    "    print(f\"- {output.name}: {output.type.tensor_type.elem_type}\")\n",
    "    shape = [dim.dim_value for dim in output.type.tensor_type.shape.dim]\n",
    "    print(f\"  Shape: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operators used in the model:\n",
      "- Add\n",
      "- Conv\n",
      "- MatMul\n",
      "- MaxPool\n",
      "- Relu\n",
      "- Reshape\n",
      "\n",
      "Initializers (weights and biases):\n",
      "- Parameter193:\n",
      "  Shape: (16, 4, 4, 10)\n",
      "  Data type: float32\n",
      "- Parameter87:\n",
      "  Shape: (16, 8, 5, 5)\n",
      "  Data type: float32\n",
      "- Parameter5:\n",
      "  Shape: (8, 1, 5, 5)\n",
      "  Data type: float32\n",
      "- Parameter6:\n",
      "  Shape: (8, 1, 1)\n",
      "  Data type: float32\n",
      "- Parameter88:\n",
      "  Shape: (16, 1, 1)\n",
      "  Data type: float32\n",
      "- Pooling160_Output_0_reshape0_shape:\n",
      "  Shape: (2,)\n",
      "  Data type: int64\n",
      "- Parameter193_reshape1_shape:\n",
      "  Shape: (2,)\n",
      "  Data type: int64\n",
      "- Parameter194:\n",
      "  Shape: (1, 10)\n",
      "  Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Print operators used in the model\n",
    "print(\"Operators used in the model:\")\n",
    "ops = {node.op_type for node in graph.node}\n",
    "for op in sorted(ops):\n",
    "    print(f\"- {op}\")\n",
    "\n",
    "# Print initializers (weights and biases)\n",
    "print(\"\\nInitializers (weights and biases):\")\n",
    "for init in graph.initializer:\n",
    "    # Convert to numpy array to get shape and data type\n",
    "    np_array = onnx.numpy_helper.to_array(init)\n",
    "    print(f\"- {init.name}:\")\n",
    "    print(f\"  Shape: {np_array.shape}\")\n",
    "    print(f\"  Data type: {np_array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'data/mnist-12.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netron.start(\"data/mnist-12.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
