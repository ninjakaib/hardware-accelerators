{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Hardware Blocks for Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrtl\n",
    "from pyrtl import *\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Type, Literal, Optional\n",
    "from hardware_accelerators.dtypes import *\n",
    "from hardware_accelerators.rtllib import *\n",
    "from hardware_accelerators.rtllib.processing_element import ProcessingElement\n",
    "from hardware_accelerators.rtllib.adders import *\n",
    "from hardware_accelerators.rtllib.multipliers import *\n",
    "from hardware_accelerators.rtllib.lmul import *\n",
    "from hardware_accelerators.rtllib.utils.common import *\n",
    "from hardware_accelerators.simulation.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_list = [Float8, BF16, Float32]\n",
    "dtype_map = {8: Float8, 16: BF16, 32: Float32}\n",
    "w_a_pairs = [(8, 8), (8, 16), (8, 32), (16, 16), (16, 32), (32, 32)]\n",
    "w_a_dtypes = [(dtype_map[w], dtype_map[a]) for w, a in w_a_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs(*bitwidths, **named_bitwidths):\n",
    "    \"\"\"\n",
    "    Create PyRTL Input wires with specified bitwidths.\n",
    "\n",
    "    Args:\n",
    "        *bitwidths: Variable number of bitwidths for unnamed inputs\n",
    "        **named_bitwidths: Named bitwidths where the key is used as the wire name\n",
    "\n",
    "    Returns:\n",
    "        Generator of PyRTL Input wires\n",
    "\n",
    "    Note:\n",
    "        You must use either all positional arguments or all keyword arguments, not a mix.\n",
    "    \"\"\"\n",
    "    if bitwidths and named_bitwidths:\n",
    "        raise ValueError(\n",
    "            \"Please use either all positional arguments or all keyword arguments, not a mix.\"\n",
    "        )\n",
    "\n",
    "    # If using positional arguments\n",
    "    for bitwidth in bitwidths:\n",
    "        yield pyrtl.Input(bitwidth)\n",
    "\n",
    "    # If using keyword arguments\n",
    "    for name, bitwidth in named_bitwidths.items():\n",
    "        yield pyrtl.Input(bitwidth, name=name)\n",
    "\n",
    "\n",
    "def create_outputs(*args, **named_wires):\n",
    "    \"\"\"\n",
    "    Create PyRTL Output wires connected to the input wires.\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of wires to connect to unnamed outputs\n",
    "        **named_wires: Named wires where the key is used as the output wire name\n",
    "\n",
    "    Note:\n",
    "        You must use either all positional arguments or all keyword arguments, not a mix.\n",
    "    \"\"\"\n",
    "    if args and named_wires:\n",
    "        raise ValueError(\n",
    "            \"Please use either all positional arguments or all keyword arguments, not a mix.\"\n",
    "        )\n",
    "\n",
    "    # If using positional arguments\n",
    "    for wire in args:\n",
    "        out = pyrtl.Output(len(wire))\n",
    "        out <<= wire\n",
    "\n",
    "    # If using keyword arguments\n",
    "    for name, wire in named_wires.items():\n",
    "        out = pyrtl.Output(len(wire), name=name)\n",
    "        out <<= wire\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RTLAnalysis:\n",
    "    \"\"\"Results of RTL analysis.\"\"\"\n",
    "\n",
    "    max_delay: float\n",
    "    max_freq: float\n",
    "    logic_area: float\n",
    "    mem_area: float\n",
    "    name: Optional[str] = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.name is None:\n",
    "            return (\n",
    "                f\"RTLAnalysisResults(\"\n",
    "                f\"max_delay={self.max_delay:.2f} ps, \"\n",
    "                f\"max_freq={self.max_freq:.2f} MHz, \"\n",
    "                f\"logic_area={self.logic_area:.2f}um², \"\n",
    "                f\"mem_area={self.mem_area:.2f}um²)\"\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                f\"RTLAnalysisResults for {self.name}:\\n\\t\"\n",
    "                f\"max_delay={self.max_delay:.2f} ps\\n\\t\"\n",
    "                f\"max_freq={self.max_freq:.2f} MHz\\n\\t\"\n",
    "                f\"logic_area={self.logic_area:.2f}um²\\n\\t\"\n",
    "                f\"mem_area={self.mem_area:.2f}um²\"\n",
    "            )\n",
    "\n",
    "\n",
    "def analyze(\n",
    "    block: Block | None = None, synth: bool = True, opt: bool = True, name=None\n",
    "):\n",
    "    if block is not None:\n",
    "        pyrtl.set_working_block(block)\n",
    "\n",
    "    if synth:\n",
    "        pyrtl.synthesize()\n",
    "    if opt:\n",
    "        pyrtl.optimize()\n",
    "\n",
    "    timing = pyrtl.TimingAnalysis()\n",
    "    max_delay = timing.max_length()\n",
    "    max_freq = timing.max_freq()\n",
    "    logic_area, mem_area = pyrtl.area_estimation()\n",
    "\n",
    "    return RTLAnalysis(\n",
    "        name=name,\n",
    "        max_delay=max_delay,\n",
    "        max_freq=max_freq,\n",
    "        logic_area=logic_area * 1e6,\n",
    "        mem_area=mem_area * 1e6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adder_blocks(dtype: Type[BaseFloat], fast: bool) -> dict[str, Block]:\n",
    "    bits = dtype.bitwidth()\n",
    "    e_bits, m_bits = dtype.exponent_bits(), dtype.mantissa_bits()\n",
    "\n",
    "    combinational_block = pyrtl.Block()\n",
    "    adder_pipelined_block = pyrtl.Block()\n",
    "    stage_2_block = pyrtl.Block()\n",
    "    stage_3_block = pyrtl.Block()\n",
    "    stage_4_block = pyrtl.Block()\n",
    "    stage_5_block = pyrtl.Block()\n",
    "\n",
    "    # Combinational design\n",
    "    with set_working_block(combinational_block):\n",
    "        create_outputs(*float_adder(*create_inputs(bits, bits), dtype=dtype, fast=fast))\n",
    "\n",
    "    # Complete pipelined design\n",
    "    with set_working_block(adder_pipelined_block):\n",
    "        create_outputs(\n",
    "            float_adder_pipelined(\n",
    "                *create_inputs(bits, bits),\n",
    "                dtype=dtype,\n",
    "                fast=fast,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Stages 1 & 2\n",
    "    with set_working_block(stage_2_block):\n",
    "        float_components = extract_float_components(\n",
    "            *create_inputs(bits, bits),\n",
    "            e_bits=e_bits,\n",
    "            m_bits=m_bits,\n",
    "        )\n",
    "        stage_2_outputs = adder_stage_2(\n",
    "            *float_components,\n",
    "            e_bits,\n",
    "            m_bits,\n",
    "            fast,\n",
    "        )\n",
    "        create_outputs(*stage_2_outputs)\n",
    "\n",
    "    # Stage 3\n",
    "    with set_working_block(stage_3_block):\n",
    "        # Perform alignment and generate SGR bits\n",
    "        stage_3_outputs = adder_stage_3(\n",
    "            *create_inputs(m_bits + 1, e_bits),\n",
    "            e_bits=e_bits,\n",
    "            m_bits=m_bits,\n",
    "        )\n",
    "        create_outputs(*stage_3_outputs)\n",
    "\n",
    "    # Stage 4\n",
    "    with set_working_block(stage_4_block):\n",
    "        # Perform mantissa addition and leading zero detection\n",
    "        stage_4_outputs = adder_stage_4(\n",
    "            *create_inputs(m_bits + 1, m_bits + 1, 1), m_bits=m_bits, fast=fast\n",
    "        )\n",
    "        create_outputs(*stage_4_outputs)\n",
    "\n",
    "    # Stage 5\n",
    "    with set_working_block(stage_5_block):\n",
    "        # Perform normalization, rounding, and final assembly\n",
    "        stage_5_outputs = adder_stage_5(\n",
    "            *create_inputs(\n",
    "                m_bits + 2,  # abs_mantissa: m_bits + 2 wide\n",
    "                1,  # sticky_bit: 1 bit\n",
    "                1,  # guard_bit: 1 bit\n",
    "                1,  # round_bit: 1 bit\n",
    "                4,  # lzc: 4 bits wide\n",
    "                e_bits,  # exp_larger: e_bits wide\n",
    "                1,  # sign_a: 1 bit\n",
    "                1,  # sign_b: 1 bit\n",
    "                e_bits + 1,  # exp_diff: e_bits + 1 wide\n",
    "                1,  # is_neg: 1 bit\n",
    "            ),\n",
    "            e_bits=e_bits,\n",
    "            m_bits=m_bits,\n",
    "        )\n",
    "        create_outputs(*stage_5_outputs)\n",
    "\n",
    "    # Return all the generated blocks for analysis\n",
    "    return {\n",
    "        \"adder_combinational\": combinational_block,\n",
    "        \"adder_pipelined\": adder_pipelined_block,\n",
    "        \"adder_stage_2\": stage_2_block,\n",
    "        \"adder_stage_3\": stage_3_block,\n",
    "        \"adder_stage_4\": stage_4_block,\n",
    "        \"adder_stage_5\": stage_5_block,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTLAnalysisResults for adder_combinational:\n",
      "\tmax_delay=4106.58 ps\n",
      "\tmax_freq=222.74 MHz\n",
      "\tlogic_area=3568.44um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for adder_pipelined:\n",
      "\tmax_delay=1908.24 ps\n",
      "\tmax_freq=436.44 MHz\n",
      "\tlogic_area=6453.85um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for adder_stage_2:\n",
      "\tmax_delay=1230.84 ps\n",
      "\tmax_freq=619.64 MHz\n",
      "\tlogic_area=818.93um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for adder_stage_3:\n",
      "\tmax_delay=1565.30 ps\n",
      "\tmax_freq=513.27 MHz\n",
      "\tlogic_area=496.58um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for adder_stage_4:\n",
      "\tmax_delay=1445.68 ps\n",
      "\tmax_freq=546.84 MHz\n",
      "\tlogic_area=1190.06um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for adder_stage_5:\n",
      "\tmax_delay=1711.24 ps\n",
      "\tmax_freq=477.50 MHz\n",
      "\tlogic_area=1080.29um²\n",
      "\tmem_area=0.00um² \n",
      "\n"
     ]
    }
   ],
   "source": [
    "adder_blocks = create_adder_blocks(Float8, fast=True)\n",
    "\n",
    "for name, block in adder_blocks.items():\n",
    "    results = analyze(block, name=name)\n",
    "    print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiplier_blocks(dtype: Type[BaseFloat], fast: bool) -> dict[str, Block]:\n",
    "    bits = dtype.bitwidth()\n",
    "    e_bits, m_bits = dtype.exponent_bits(), dtype.mantissa_bits()\n",
    "\n",
    "    combinational_block = pyrtl.Block()\n",
    "    multiplier_block = pyrtl.Block()\n",
    "    stage_2_block = pyrtl.Block()\n",
    "    stage_3_block = pyrtl.Block()\n",
    "    stage_4_block = pyrtl.Block()\n",
    "\n",
    "    # Combinational design\n",
    "    with set_working_block(combinational_block):\n",
    "        create_outputs(\n",
    "            float_multiplier(*create_inputs(bits, bits), dtype=dtype, fast=fast)\n",
    "        )\n",
    "\n",
    "    # Complete pipelined design\n",
    "    with set_working_block(multiplier_block):\n",
    "        multiplier = FloatMultiplierPipelined(\n",
    "            *create_inputs(bits, bits), dtype=dtype, fast=fast\n",
    "        )\n",
    "        create_outputs(multiplier._result)\n",
    "\n",
    "    # Stage 1 & 2: Extract components and calculate sign, exponent sum, mantissa product\n",
    "    with set_working_block(stage_2_block):\n",
    "        float_components = extract_float_components(\n",
    "            *create_inputs(bits, bits),\n",
    "            e_bits=e_bits,\n",
    "            m_bits=m_bits,\n",
    "        )\n",
    "        stage_2_outputs = multiplier_stage_2(\n",
    "            *float_components,\n",
    "            m_bits,\n",
    "            fast,\n",
    "        )\n",
    "        create_outputs(*stage_2_outputs)\n",
    "\n",
    "    # Stage 3: Leading zero detection and exponent adjustment\n",
    "    with set_working_block(stage_3_block):\n",
    "        stage_3_outputs = multiplier_stage_3(\n",
    "            *create_inputs(e_bits + 1, 2 * m_bits + 2),  # exp_sum, mantissa_product\n",
    "            e_bits=e_bits,\n",
    "            m_bits=m_bits,\n",
    "            fast=fast,\n",
    "        )\n",
    "        create_outputs(*stage_3_outputs)\n",
    "\n",
    "    # Stage 4: Normalization, rounding, and final assembly\n",
    "    with set_working_block(stage_4_block):\n",
    "        stage_4_outputs = multiplier_stage_4(\n",
    "            *create_inputs(\n",
    "                e_bits,  # unbiased_exp\n",
    "                e_bits,  # leading_zeros\n",
    "                2 * m_bits + 2,  # mantissa_product\n",
    "            ),\n",
    "            m_bits=m_bits,\n",
    "            e_bits=e_bits,\n",
    "            fast=fast,\n",
    "        )\n",
    "        create_outputs(*stage_4_outputs)\n",
    "\n",
    "    # Return all the generated blocks for analysis\n",
    "    return {\n",
    "        \"combinational\": combinational_block,\n",
    "        \"multiplier\": multiplier_block,\n",
    "        \"stage_2\": stage_2_block,\n",
    "        \"stage_3\": stage_3_block,\n",
    "        \"stage_4\": stage_4_block,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTLAnalysisResults for combinational:\n",
      "\tmax_delay=4906.51 ps\n",
      "\tmax_freq=189.05 MHz\n",
      "\tlogic_area=3023.06um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for multiplier:\n",
      "\tmax_delay=1828.47 ps\n",
      "\tmax_freq=452.19 MHz\n",
      "\tlogic_area=3800.17um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for stage_2:\n",
      "\tmax_delay=1394.44 ps\n",
      "\tmax_freq=562.61 MHz\n",
      "\tlogic_area=1062.86um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for stage_3:\n",
      "\tmax_delay=1585.10 ps\n",
      "\tmax_freq=508.10 MHz\n",
      "\tlogic_area=670.82um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for stage_4:\n",
      "\tmax_delay=1828.47 ps\n",
      "\tmax_freq=452.19 MHz\n",
      "\tlogic_area=1210.97um²\n",
      "\tmem_area=0.00um² \n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiplier_blocks = create_multiplier_blocks(Float8, fast=True)\n",
    "\n",
    "for name, block in multiplier_blocks.items():\n",
    "    results = analyze(block, name=name)\n",
    "    print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-mul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lmul_blocks(dtype: Type[BaseFloat]) -> dict[str, Block]:\n",
    "    bits = dtype.bitwidth()\n",
    "\n",
    "    combinational_block = pyrtl.Block()\n",
    "    combinational_fast_block = pyrtl.Block()\n",
    "    pipelined_block = pyrtl.Block()\n",
    "    pipelined_fast_block = pyrtl.Block()\n",
    "\n",
    "    # Combinational design (simple)\n",
    "    with set_working_block(combinational_block):\n",
    "        create_outputs(lmul_simple(*create_inputs(bits, bits), dtype=dtype))\n",
    "\n",
    "    # Combinational design (fast)\n",
    "    with set_working_block(combinational_fast_block):\n",
    "        create_outputs(lmul_fast(*create_inputs(bits, bits), dtype=dtype))\n",
    "\n",
    "    # Pipelined design (simple)\n",
    "    with set_working_block(pipelined_block):\n",
    "        mult = LmulPipelined(*create_inputs(bits, bits), dtype=dtype, fast=False)\n",
    "        create_outputs(mult.output_reg)\n",
    "\n",
    "    # Pipelined design (fast)\n",
    "    with set_working_block(pipelined_fast_block):\n",
    "        mult = LmulPipelined(*create_inputs(bits, bits), dtype=dtype, fast=True)\n",
    "        create_outputs(mult.output_reg)\n",
    "\n",
    "    # Return all the generated blocks for analysis\n",
    "    return {\n",
    "        \"combinational_simple\": combinational_block,\n",
    "        \"combinational_fast\": combinational_fast_block,\n",
    "        \"pipelined_simple\": pipelined_block,\n",
    "        \"pipelined_fast\": pipelined_fast_block,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTLAnalysisResults for combinational_simple:\n",
      "\tmax_delay=1962.64 ps\n",
      "\tmax_freq=426.32 MHz\n",
      "\tlogic_area=635.98um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for combinational_fast:\n",
      "\tmax_delay=1406.37 ps\n",
      "\tmax_freq=558.86 MHz\n",
      "\tlogic_area=1036.73um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for pipelined_simple:\n",
      "\tmax_delay=2223.24 ps\n",
      "\tmax_freq=383.69 MHz\n",
      "\tlogic_area=1998.53um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for pipelined_fast:\n",
      "\tmax_delay=1085.34 ps\n",
      "\tmax_freq=681.04 MHz\n",
      "\tlogic_area=2103.08um²\n",
      "\tmem_area=0.00um² \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lmul_blocks = create_lmul_blocks(Float8)\n",
    "\n",
    "for name, block in lmul_blocks.items():\n",
    "    results = analyze(block, name=name)\n",
    "    print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_pe_io(pe: ProcessingElement):\n",
    "    # Connect the inputs and outputs of the processing element\n",
    "    w_bits, a_bits = pe.weight_type.bitwidth(), pe.data_type.bitwidth()\n",
    "    w_in, d_in, acc_in = create_inputs(\n",
    "        weight_in=w_bits, data_in=a_bits, accum_in=a_bits\n",
    "    )\n",
    "    # w_in, d_in, acc_in = create_inputs(w_bits, a_bits, a_bits)\n",
    "    pe.connect_weight(w_in)\n",
    "    pe.connect_data(d_in)\n",
    "    pe.connect_accum(acc_in)\n",
    "    # if pe.pipeline:\n",
    "    #     controls = create_inputs(weight_en=1, data_en=1, mul_en=1, adder_en=1)\n",
    "    pe.connect_control_signals(\n",
    "        *create_inputs(weight_en=1, data_en=1, mul_en=1, adder_en=1)\n",
    "    )\n",
    "    create_outputs(*pe.outputs.__dict__.values())\n",
    "\n",
    "\n",
    "def create_pe_blocks(\n",
    "    dtypes: tuple[Type[BaseFloat], Type[BaseFloat]]\n",
    ") -> dict[str, Block]:\n",
    "    \"\"\"Create a processing element for each pair of dtypes.\"\"\"\n",
    "\n",
    "    weight_dtype, act_dtype = dtypes\n",
    "\n",
    "    # Defining blocks to encapsulate hardware\n",
    "\n",
    "    combinational_block = Block()\n",
    "    simple_pipeline_block = Block()\n",
    "    simple_pipeline_fast_block = Block()\n",
    "    full_pipeline_block = Block()\n",
    "    full_pipeline_fast_block = Block()\n",
    "\n",
    "    combinational_lmul_block = Block()\n",
    "    simple_pipeline_lmul_block = Block()\n",
    "    simple_pipeline_fast_lmul_block = Block()\n",
    "    full_pipeline_lmul_block = Block()\n",
    "    full_pipeline_fast_lmul_block = Block()\n",
    "\n",
    "    # Standard IEEE multiplier versions\n",
    "\n",
    "    with set_working_block(combinational_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=float_multiplier,\n",
    "            adder=float_adder,\n",
    "            pipeline_mult=False,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(simple_pipeline_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=float_multiplier,\n",
    "            adder=float_adder,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(simple_pipeline_fast_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=float_multiplier_fast_unstable,\n",
    "            adder=float_adder_fast_unstable,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(full_pipeline_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=float_multiplier_pipelined,\n",
    "            adder=float_adder_pipelined,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(full_pipeline_fast_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=float_multiplier_pipelined_fast_unstable,\n",
    "            adder=float_adder_pipelined_fast_unstable,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    # L-mul versions\n",
    "\n",
    "    with set_working_block(combinational_lmul_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=lmul_simple,\n",
    "            adder=float_adder,\n",
    "            pipeline_mult=False,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(simple_pipeline_lmul_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=lmul_simple,\n",
    "            adder=float_adder,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(simple_pipeline_fast_lmul_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=lmul_fast,\n",
    "            adder=float_adder_fast_unstable,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(full_pipeline_lmul_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=lmul_pipelined,\n",
    "            adder=float_adder_pipelined,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    with set_working_block(full_pipeline_fast_lmul_block):\n",
    "        pe = ProcessingElement(\n",
    "            data_type=act_dtype,\n",
    "            weight_type=weight_dtype,\n",
    "            accum_type=act_dtype,\n",
    "            multiplier=lmul_pipelined_fast,\n",
    "            adder=float_adder_pipelined_fast_unstable,\n",
    "            pipeline_mult=True,\n",
    "        )\n",
    "        connect_pe_io(pe)\n",
    "\n",
    "    return {\n",
    "        \"combinational\": combinational_block,\n",
    "        \"standard\": simple_pipeline_block,\n",
    "        \"fast\": simple_pipeline_fast_block,\n",
    "        \"pipelined\": full_pipeline_block,\n",
    "        \"fast_pipelined\": full_pipeline_fast_block,\n",
    "        \"combinational_lmul\": combinational_lmul_block,\n",
    "        \"standard_lmul\": simple_pipeline_lmul_block,\n",
    "        \"fast_lmul\": simple_pipeline_fast_lmul_block,\n",
    "        \"pipelined_lmul\": full_pipeline_lmul_block,\n",
    "        \"fast_pipelined_lmul\": full_pipeline_fast_lmul_block,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelining disabled, no product register to enable. Deleting wire.\n",
      "Pipelining disabled, no product register to enable. Deleting wire.\n",
      "RTLAnalysisResults for combinational:\n",
      "\tmax_delay=12953.76 ps\n",
      "\tmax_freq=74.98 MHz\n",
      "\tlogic_area=7992.39um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for standard:\n",
      "\tmax_delay=7848.58 ps\n",
      "\tmax_freq=121.48 MHz\n",
      "\tlogic_area=8452.38um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for fast:\n",
      "\tmax_delay=5110.31 ps\n",
      "\tmax_freq=182.04 MHz\n",
      "\tlogic_area=8431.47um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for pipelined:\n",
      "\tmax_delay=3226.71 ps\n",
      "\tmax_freq=277.03 MHz\n",
      "\tlogic_area=12114.91um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for fast_pipelined:\n",
      "\tmax_delay=2112.04 ps\n",
      "\tmax_freq=400.80 MHz\n",
      "\tlogic_area=12094.00um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for combinational_lmul:\n",
      "\tmax_delay=8835.42 ps\n",
      "\tmax_freq=108.48 MHz\n",
      "\tlogic_area=5596.59um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for standard_lmul:\n",
      "\tmax_delay=7848.58 ps\n",
      "\tmax_freq=121.48 MHz\n",
      "\tlogic_area=6056.58um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for fast_lmul:\n",
      "\tmax_delay=4310.38 ps\n",
      "\tmax_freq=213.07 MHz\n",
      "\tlogic_area=6445.14um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for pipelined_lmul:\n",
      "\tmax_delay=3226.71 ps\n",
      "\tmax_freq=277.03 MHz\n",
      "\tlogic_area=10304.55um²\n",
      "\tmem_area=0.00um² \n",
      "\n",
      "RTLAnalysisResults for fast_pipelined_lmul:\n",
      "\tmax_delay=2112.04 ps\n",
      "\tmax_freq=400.80 MHz\n",
      "\tlogic_area=10396.90um²\n",
      "\tmem_area=0.00um² \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reset_working_block()\n",
    "pe_blocks = create_pe_blocks((Float8, Float8))\n",
    "\n",
    "for name, block in pe_blocks.items():\n",
    "    results = analyze(block, name=name)\n",
    "    print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New top level class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AcceleratorAnalysisConfig:\n",
    "    \"\"\"Configuration for an accelerator to be generated for analysis.\"\"\"\n",
    "\n",
    "    array_size: int\n",
    "    \"\"\"\n",
    "    The size of the systolic array (N x N).\n",
    "    Determines the number of processing elements in the accelerator.\n",
    "    \"\"\"\n",
    "\n",
    "    weight_type: Type[BaseFloat]\n",
    "    \"\"\"\n",
    "    The floating-point data type for weights.\n",
    "    Must be a subclass of BaseFloat (e.g., Float8, BF16, Float32).\n",
    "    \"\"\"\n",
    "\n",
    "    activation_type: Type[BaseFloat]\n",
    "    \"\"\"\n",
    "    The floating-point data type for activations/inputs.\n",
    "    Must be a subclass of BaseFloat (e.g., Float8, BF16, Float32).\n",
    "    \"\"\"\n",
    "\n",
    "    lmul: bool\n",
    "    \"\"\"\n",
    "    Whether to use L-mul for multiplication operations.\n",
    "    If True, uses linear-time multipliers; if False, uses standard IEEE multipliers.\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline_level: Literal[\"low\", \"high\"] | None\n",
    "    \"\"\"\n",
    "    The level of pipelining in the accelerator:\n",
    "    - None: No pipelining (fully combinational design)\n",
    "    - 'low': Basic pipelining between multiplier and adder in each PE\n",
    "    - 'high': Full pipelining with pipelined arithmetic units\n",
    "    \"\"\"\n",
    "\n",
    "    use_fast_internals: bool\n",
    "    \"\"\"\n",
    "    Whether to use faster basic arithmetic implementations with more complex low-level RTL.  \n",
    "    - True: uses optimized arithmetic units from PyRTL's rtllib  \n",
    "    - False: prioritize simplicity over speed  \n",
    "\n",
    "    WARNING: Setting to True could potentially make final synthesis on the Verilog output worse as the synthesis tools will not be able to infer optimal circuits from the complex low-level RTL.\n",
    "    \"\"\"\n",
    "\n",
    "    accum_addr_width: int = 12\n",
    "    \"\"\"\n",
    "    The bit width of the accumulator address.\n",
    "    Determines the size of the accumulator memory (2^width entries).\n",
    "    Default is 12 bits (4096 entries).\n",
    "    \"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Ensure activation dtype has bitwidth >= weight dtype\n",
    "        if self.activation_type.bitwidth() < self.weight_type.bitwidth():\n",
    "            raise ValueError(\n",
    "                f\"Activation dtype bitwidth ({self.activation_type.bitwidth()}) must be greater than or equal to \"\n",
    "                f\"weight dtype bitwidth ({self.weight_type.bitwidth()})\"\n",
    "            )\n",
    "\n",
    "        # Determine if we should use pipelined arithmetic functions\n",
    "        use_pipelined_funcs = self.pipeline_level == \"high\"\n",
    "\n",
    "        # Set pipeline_pe flag for PE configuration\n",
    "        # True if any pipeline level is specified (low or high)\n",
    "        self.pipeline_pe = self.pipeline_level is not None\n",
    "\n",
    "        # Multiplier function selection using dictionary mapping\n",
    "        multiplier_map = {\n",
    "            # (lmul, use_pipelined_funcs, fast_internals) -> function\n",
    "            (True, True, True): lmul_pipelined_fast,\n",
    "            (True, True, False): lmul_pipelined,\n",
    "            (True, False, True): lmul_fast,\n",
    "            (True, False, False): lmul_simple,\n",
    "            (False, True, True): float_multiplier_pipelined_fast_unstable,\n",
    "            (False, True, False): float_multiplier_pipelined,\n",
    "            (False, False, True): float_multiplier_fast_unstable,\n",
    "            (False, False, False): float_multiplier,\n",
    "        }\n",
    "\n",
    "        # Adder function selection using dictionary mapping\n",
    "        adder_map = {\n",
    "            # (use_pipelined_funcs, fast_internals) -> function\n",
    "            (True, True): float_adder_pipelined_fast_unstable,\n",
    "            (True, False): float_adder_pipelined,\n",
    "            (False, True): float_adder_fast_unstable,\n",
    "            (False, False): float_adder,\n",
    "        }\n",
    "\n",
    "        # Select functions using the maps\n",
    "        self.multiplier_func = multiplier_map[\n",
    "            (self.lmul, use_pipelined_funcs, self.use_fast_internals)\n",
    "        ]\n",
    "        self.adder_func = adder_map[(use_pipelined_funcs, self.use_fast_internals)]\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        dtype_name = lambda d: d.bitwidth() if d != BF16 else \"b16\"\n",
    "        mul = \"-lmul\" if self.lmul else \"-ieee\"\n",
    "        pipe_name_map = {\"low\": \"-pipePE\", \"high\": \"-pipeALL\"}\n",
    "        fast = \"-fast\" if self.use_fast_internals else \"\"\n",
    "        mem = f\"-m{self.accum_addr_width}\" if self.accum_addr_width != 12 else \"\"\n",
    "        return (\n",
    "            f\"w{dtype_name(self.weight_type)}\"\n",
    "            f\"a{dtype_name(self.activation_type)}\"\n",
    "            f\"-{self.array_size}x{self.array_size}\"\n",
    "            + mem\n",
    "            + mul\n",
    "            + fast\n",
    "            + pipe_name_map.get(self.pipeline_level, \"\")  # type: ignore\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hardware_accelerators.rtllib.accumulators import Accumulator\n",
    "from hardware_accelerators.rtllib.activations import ReluUnit\n",
    "\n",
    "\n",
    "class AcceleratorTopLevel(CompiledAccelerator):\n",
    "    def __init__(self, config: AcceleratorAnalysisConfig):\n",
    "        self.config = config\n",
    "\n",
    "        # Instantiate hardware components\n",
    "        self.systolic_array = SystolicArrayDiP(\n",
    "            size=config.array_size,\n",
    "            data_type=config.activation_type,\n",
    "            weight_type=config.weight_type,\n",
    "            accum_type=config.activation_type,\n",
    "            multiplier=config.multiplier_func,\n",
    "            adder=config.adder_func,\n",
    "            pipeline=config.pipeline_pe,\n",
    "        )\n",
    "        self.accumulator = Accumulator(\n",
    "            addr_width=12,\n",
    "            array_size=config.array_size,\n",
    "            data_type=config.activation_type,\n",
    "            adder=config.adder_func,\n",
    "        )\n",
    "        self.activation = ReluUnit(\n",
    "            size=config.array_size,\n",
    "            dtype=config.activation_type,\n",
    "        )\n",
    "        self.outputs = [\n",
    "            Output(config.activation_type.bitwidth(), f\"out_{i}\")\n",
    "            for i in range(config.array_size)\n",
    "        ]\n",
    "\n",
    "        # Connect everything together and create io ports\n",
    "        self._connect_components()\n",
    "        self.valid_out = Output(1, \"valid_out\")\n",
    "        self.valid_out <<= self.activation.outputs_valid\n",
    "\n",
    "    def _create_control_wires(self):\n",
    "        \"\"\"Create named Input wires for control signals\"\"\"\n",
    "        self.data_enable = Input(1, \"data_enable\")\n",
    "        self.data_ins = [\n",
    "            Input(self.config.activation_type.bitwidth(), f\"data_in_{i}\")\n",
    "            for i in range(self.config.array_size)\n",
    "        ]\n",
    "        self.weight_enable = Input(1, \"weight_enable\")\n",
    "        self.weights_in = [\n",
    "            Input(self.config.weight_type.bitwidth(), f\"weight_in_{i}\")\n",
    "            for i in range(self.config.array_size)\n",
    "        ]\n",
    "        self.accum_addr_in = Input(self.config.accum_addr_width, \"accum_addr_in\")\n",
    "        self.accum_mode_in = Input(1, \"accum_mode_in\")\n",
    "        self.act_start_in = Input(1, \"act_start_in\")\n",
    "        self.act_func_in = Input(1, \"act_func_in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware generation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def create_accelerator_blocks(\n",
    "    dtypes: tuple[Type[BaseFloat], Type[BaseFloat]],\n",
    "    array_size: int = 4,\n",
    "    addr_bits: int = 12,\n",
    ") -> dict[str, Block]:\n",
    "    \"\"\"\n",
    "    Create accelerator blocks for all valid configurations based on the given inputs.\n",
    "\n",
    "    Args:\n",
    "        dtypes: Tuple of (weight_type, activation_type) data types\n",
    "        array_size: Size of the systolic array (N x N)\n",
    "        addr_bits: Bit width for accumulator address (uses default if None)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping configuration names to PyRTL blocks\n",
    "    \"\"\"\n",
    "    weight_type, activation_type = dtypes\n",
    "\n",
    "    # Define all valid configurations to test\n",
    "    pipeline_options = [None, \"low\", \"high\"]\n",
    "    lmul_options = [False, True]\n",
    "    fast_options = [False, True]\n",
    "\n",
    "    # Create configs and blocks\n",
    "    blocks = {}\n",
    "    for pipeline, lmul, fast in product(pipeline_options, lmul_options, fast_options):\n",
    "        if pipeline is None and fast is True:\n",
    "            continue\n",
    "\n",
    "        # Create the configuration\n",
    "        config = AcceleratorAnalysisConfig(\n",
    "            array_size=array_size,\n",
    "            activation_type=activation_type,\n",
    "            weight_type=weight_type,\n",
    "            lmul=lmul,\n",
    "            accum_addr_width=addr_bits,\n",
    "            pipeline_level=pipeline,\n",
    "            use_fast_internals=fast,\n",
    "        )\n",
    "\n",
    "        block = pyrtl.Block()\n",
    "        with set_working_block(block):\n",
    "            AcceleratorTopLevel(config)\n",
    "\n",
    "        blocks[config.name] = block\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTLAnalysisResults for w8a8-2x2-ieee:\n",
      "\tmax_delay=12953.76 ps\n",
      "\tmax_freq=74.98 MHz\n",
      "\tlogic_area=40216.33um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-lmul:\n",
      "\tmax_delay=8835.42 ps\n",
      "\tmax_freq=108.48 MHz\n",
      "\tlogic_area=30633.13um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-ieee-pipePE:\n",
      "\tmax_delay=8385.19 ps\n",
      "\tmax_freq=114.05 MHz\n",
      "\tlogic_area=42589.48um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-ieee-fast-pipePE:\n",
      "\tmax_delay=5110.31 ps\n",
      "\tmax_freq=182.04 MHz\n",
      "\tlogic_area=44432.94um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-lmul-pipePE:\n",
      "\tmax_delay=8385.19 ps\n",
      "\tmax_freq=114.05 MHz\n",
      "\tlogic_area=33006.28um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-lmul-fast-pipePE:\n",
      "\tmax_delay=4846.99 ps\n",
      "\tmax_freq=191.21 MHz\n",
      "\tlogic_area=36487.60um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-ieee-pipeALL:\n",
      "\tmax_delay=3226.71 ps\n",
      "\tmax_freq=277.03 MHz\n",
      "\tlogic_area=59400.16um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-ieee-fast-pipeALL:\n",
      "\tmax_delay=2112.04 ps\n",
      "\tmax_freq=400.80 MHz\n",
      "\tlogic_area=62749.05um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-lmul-pipeALL:\n",
      "\tmax_delay=3226.71 ps\n",
      "\tmax_freq=277.03 MHz\n",
      "\tlogic_area=53538.72um²\n",
      "\tmem_area=561065.91um² \n",
      "\n",
      "RTLAnalysisResults for w8a8-2x2-lmul-fast-pipeALL:\n",
      "\tmax_delay=2112.04 ps\n",
      "\tmax_freq=400.80 MHz\n",
      "\tlogic_area=57340.64um²\n",
      "\tmem_area=561065.91um² \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator_blocks = create_accelerator_blocks((Float8, Float8), 2, 12)\n",
    "\n",
    "for name, block in accelerator_blocks.items():\n",
    "    results = analyze(block, name=name)\n",
    "    print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = \"MB\"\n",
    "addr_bits = 12\n",
    "dtype = Float8\n",
    "array_size = 256\n",
    "\n",
    "desired_mem = 2.1\n",
    "\n",
    "mem = calculate_accumulator_memory(addr_bits, array_size, dtype, unit, True)\n",
    "min_addr_bits_mem = calculate_accum_addr_width_for_min_mem(\n",
    "    desired_mem, array_size, dtype, unit, True\n",
    ")\n",
    "min_addr_bits_slots = calculate_accum_addr_width_for_min_slots(10000, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
