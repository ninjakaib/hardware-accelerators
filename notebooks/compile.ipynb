{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrtl import CompiledSimulation, reset_working_block\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from hardware_accelerators import *\n",
    "from hardware_accelerators.simulation.compile import (\n",
    "    ReusableCompiledSimulation,\n",
    "    CompiledAccelerator,\n",
    ")\n",
    "from hardware_accelerators.simulation.compile import CompiledAcceleratorSimulator\n",
    "from hardware_accelerators.rtllib.accelerator import CompiledAcceleratorConfig\n",
    "from hardware_accelerators.nn import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading from saved sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: convert images to tensor and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "# Download MNIST test data\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    batch, labels = next(iter(loader))\n",
    "    return batch.reshape(batch_size, -1).numpy(), labels.numpy()\n",
    "\n",
    "\n",
    "def get_activation():\n",
    "    image, _ = next(iter(test_loader))\n",
    "    image = image.detach().numpy().reshape(-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 4\n",
    "\n",
    "config = CompiledAcceleratorConfig(\n",
    "    array_size=CHUNK_SIZE,\n",
    "    activation_type=BF16,\n",
    "    weight_type=BF16,\n",
    "    multiplier=float_multiplier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_working_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaibreese/UCSD/dsc180b/hardware-accelerators/hardware_accelerators/nn/util.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precompiled library: /Users/kaibreese/UCSD/dsc180b/hardware-accelerators/hardware_accelerators/lib/wb16ab16s4/pyrtlsim.so\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"models/mlp_mnist.pth\")\n",
    "acc_sim = CompiledAcceleratorSimulator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.87383124e-06, 1.87383124e-06, 1.87383124e-06, 6.69541272e-06,\n",
       "       1.87383124e-06, 1.87383124e-06, 1.87383124e-06, 9.99974566e-01,\n",
       "       1.87383124e-06, 1.87383124e-06, 1.87383124e-06, 1.87383124e-06])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = acc_sim.run_mlp(model, get_activation())\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.Tensor(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyrtlError",
     "evalue": "No context available. Please run a simulation step",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPyrtlError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_sim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UCSD/dsc180b/hardware-accelerators/hardware_accelerators/simulation/compile.py:331\u001b[0m, in \u001b[0;36mCompiledAcceleratorSimulator.inspect_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minspect_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get current output values converted to floating point\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    329\u001b[0m         [\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mactivation_type(\n\u001b[0;32m--> 331\u001b[0m                 binint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m             )\u001b[38;5;241m.\u001b[39mdecimal_approx\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_wires\n\u001b[1;32m    334\u001b[0m         ]\n\u001b[1;32m    335\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dsc180/lib/python3.12/site-packages/pyrtl/compilesim.py:152\u001b[0m, in \u001b[0;36mCompiledSimulation.inspect\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vals:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PyrtlError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo context available. Please run a simulation step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vals[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PyrtlError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompiledSimulation does not support inspecting internal WireVectors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mPyrtlError\u001b[0m: No context available. Please run a simulation step"
     ]
    }
   ],
   "source": [
    "loaded_sim.inspect_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling all configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Type, List, Callable\n",
    "from itertools import product\n",
    "\n",
    "from hardware_accelerators.dtypes import *\n",
    "\n",
    "\n",
    "def generate_accelerator_configs(\n",
    "    array_size: int = 16,\n",
    "    dtypes: List[Type[BaseFloat]] = None,\n",
    "    multipliers: List[Callable] = None,\n",
    ") -> Iterator[CompiledAcceleratorConfig]:\n",
    "    \"\"\"\n",
    "    Generate all valid CompiledAcceleratorConfig combinations.\n",
    "\n",
    "    Args:\n",
    "        array_size: Size of the systolic array\n",
    "        dtypes: List of data types to consider. Defaults to [Float8, BF16, FP16, FP32]\n",
    "        multipliers: List of multiplier functions. Defaults to [float_multiplier, lmul]\n",
    "\n",
    "    Yields:\n",
    "        Valid CompiledAcceleratorConfig objects\n",
    "\n",
    "    Restrictions:\n",
    "        1. The activation_type must be greater than or equal to the weight_type in terms of bitwidth.\n",
    "        2. 16-bit float types (BF16, FP16) should not be combined with each other.\n",
    "           They should only pair with themselves or with FP32.\n",
    "    \"\"\"\n",
    "    if dtypes is None:\n",
    "        dtypes = [Float8, BF16, Float16, Float32]\n",
    "\n",
    "    if multipliers is None:\n",
    "        multipliers = [float_multiplier, lmul_fast]\n",
    "\n",
    "    # Sort dtypes by bitwidth for easier comparison\n",
    "    dtype_bitwidths = {dtype: dtype.bitwidth() for dtype in dtypes}\n",
    "    sorted_dtypes = sorted(dtypes, key=lambda d: dtype_bitwidths[d])\n",
    "\n",
    "    # Identify 16-bit float types\n",
    "    bit16_float_types = [dtype for dtype in dtypes if dtype_bitwidths[dtype] == 16]\n",
    "\n",
    "    # Generate all combinations\n",
    "    for multiplier in multipliers:\n",
    "        for weight_type in sorted_dtypes:\n",
    "            # Find valid activation types based on bitwidth\n",
    "            valid_activation_types = [\n",
    "                dtype\n",
    "                for dtype in sorted_dtypes\n",
    "                if dtype_bitwidths[dtype] >= dtype_bitwidths[weight_type]\n",
    "            ]\n",
    "\n",
    "            for activation_type in valid_activation_types:\n",
    "                # Skip invalid combinations of 16-bit float types\n",
    "                if (\n",
    "                    weight_type in bit16_float_types\n",
    "                    and activation_type in bit16_float_types\n",
    "                    and weight_type != activation_type\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                yield CompiledAcceleratorConfig(\n",
    "                    array_size=array_size,\n",
    "                    activation_type=activation_type,\n",
    "                    weight_type=weight_type,\n",
    "                    multiplier=multiplier,\n",
    "                )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def print_all_configs():\n",
    "    for i, config in enumerate(generate_accelerator_configs()):\n",
    "        print(f\"Config {i+1}:\")\n",
    "        print(f\"  Array Size: {config.array_size}\")\n",
    "        print(f\"  Activation Type: {config.activation_type.__name__}\")\n",
    "        print(f\"  Weight Type: {config.weight_type.__name__}\")\n",
    "        print(f\"  Multiplier: {config.multiplier.__name__}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 1:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float8\n",
      "  Weight Type: Float8\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 2:\n",
      "  Array Size: 16\n",
      "  Activation Type: BF16\n",
      "  Weight Type: Float8\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 3:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float16\n",
      "  Weight Type: Float8\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 4:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: Float8\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 5:\n",
      "  Array Size: 16\n",
      "  Activation Type: BF16\n",
      "  Weight Type: BF16\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 6:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: BF16\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 7:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float16\n",
      "  Weight Type: Float16\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 8:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: Float16\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 9:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: Float32\n",
      "  Multiplier: float_multiplier\n",
      "\n",
      "Config 10:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float8\n",
      "  Weight Type: Float8\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 11:\n",
      "  Array Size: 16\n",
      "  Activation Type: BF16\n",
      "  Weight Type: Float8\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 12:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float16\n",
      "  Weight Type: Float8\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 13:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: Float8\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 14:\n",
      "  Array Size: 16\n",
      "  Activation Type: BF16\n",
      "  Weight Type: BF16\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 15:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: BF16\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 16:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float16\n",
      "  Weight Type: Float16\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 17:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: Float16\n",
      "  Multiplier: lmul_fast\n",
      "\n",
      "Config 18:\n",
      "  Array Size: 16\n",
      "  Activation Type: Float32\n",
      "  Weight Type: Float32\n",
      "  Multiplier: lmul_fast\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_all_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
