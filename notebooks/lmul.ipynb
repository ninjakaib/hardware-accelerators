{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hardware_accelerators.dtypes import *\n",
    "from hardware_accelerators.rtllib.lmul import *\n",
    "from hardware_accelerators.rtllib.multipliers import *\n",
    "from hardware_accelerators.rtllib.adders import *\n",
    "from hardware_accelerators.rtllib.utils.lmul_utils import *\n",
    "from hardware_accelerators.simulation.utils import render_waveform\n",
    "from hardware_accelerators.simulation.repr_funcs import *\n",
    "from hardware_accelerators.simulation import SystolicArraySimulator\n",
    "from hardware_accelerators.nn import load_model, softmax\n",
    "from pyrtl import *\n",
    "import numpy as np\n",
    "from typing import Type\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: convert images to tensor and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "# Download MNIST test data\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    batch, labels = next(iter(loader))\n",
    "    return batch.reshape(batch_size, -1).numpy(), labels.numpy()\n",
    "\n",
    "\n",
    "def get_activation():\n",
    "    image, _ = next(iter(test_loader))\n",
    "    image = image.detach().numpy().reshape(-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyRTL lmul fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmul_fix(float_a: WireVector, float_b: WireVector, dtype: Type[BaseFloat]):\n",
    "    e_bits, m_bits = dtype.exponent_bits(), dtype.mantissa_bits()\n",
    "    em_bits = e_bits + m_bits\n",
    "    sign_a = float_a[em_bits]\n",
    "    sign_b = float_b[em_bits]\n",
    "    exp_a = float_a[m_bits:-1]\n",
    "    exp_b = float_b[m_bits:-1]\n",
    "    exp_a.name = \"exp_a\"\n",
    "    exp_b.name = \"exp_b\"\n",
    "    exp_mantissa_a = float_a[:em_bits]\n",
    "    exp_mantissa_b = float_b[:em_bits]\n",
    "\n",
    "    fp_out = WireVector(dtype.bitwidth())\n",
    "\n",
    "    OFFSET_MINUS_BIAS = pyrtl.Const(\n",
    "        get_combined_offset(e_bits, m_bits, True), bitwidth=em_bits\n",
    "    )\n",
    "\n",
    "    final_sum = carrysave_adder(\n",
    "        exp_mantissa_a, exp_mantissa_b, OFFSET_MINUS_BIAS, final_adder=kogge_stone\n",
    "    )\n",
    "\n",
    "    MAX_VALUE = pyrtl.Const(dtype.binary_max(), bitwidth=em_bits)\n",
    "\n",
    "    mantissa_result = pyrtl.mux(\n",
    "        final_sum[em_bits:],\n",
    "        0,\n",
    "        final_sum[:em_bits],\n",
    "        default=MAX_VALUE,\n",
    "    )\n",
    "    zero_or_subnormal = pyrtl.and_all_bits(exp_a) | pyrtl.and_all_bits(exp_b)\n",
    "\n",
    "    with conditional_assignment:\n",
    "        with zero_or_subnormal:\n",
    "            fp_out |= 0\n",
    "        with pyrtl.otherwise:\n",
    "            fp_out |= pyrtl.concat(sign_a ^ sign_b, mantissa_result)\n",
    "\n",
    "    return fp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b11111'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(2**5 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b100000'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1\n",
    "bin(x << 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class FastLmul:\n",
    "    def __init__(self, dtype: Type[BaseFloat]):\n",
    "        self.dtype = dtype\n",
    "        bits = dtype.bitwidth()\n",
    "        reset_working_block()\n",
    "        input_a, input_b = Input(bits, \"input_a\"), Input(bits, \"input_b\")\n",
    "        self.output = Output(bits, \"output\")\n",
    "        self.output <<= lmul_fix(input_a, input_b, dtype)\n",
    "        self.sim = pyrtl.CompiledSimulation()\n",
    "        self.lmul_offset = {\n",
    "            # BF16: 16248,\n",
    "            BF16: get_combined_offset(8, 7, True),\n",
    "            # Float32: 1064828928,\n",
    "            Float32: get_combined_offset(8, 23, True),\n",
    "        }\n",
    "        self.bitmask = {\n",
    "            BF16: 0b0111111111111111,\n",
    "            Float32: 0b01111111111111111111111111111111,\n",
    "        }\n",
    "\n",
    "    def run(self, a: float, b: float) -> float:\n",
    "        self.sim.step(\n",
    "            {\"input_a\": self.dtype(a).binint, \"input_b\": self.dtype(b).binint},\n",
    "        )\n",
    "        return float(self.dtype(binint=self.sim.inspect(\"output\")))\n",
    "\n",
    "    def __call__(self, a: float, b: float) -> Any:\n",
    "        bin_a, bin_b = self.dtype(a).binint, self.dtype(b).binint\n",
    "        sign = (bin_a >> (self.dtype.bitwidth() - 1)) ^ (\n",
    "            bin_b >> (self.dtype.bitwidth() - 1)\n",
    "        )\n",
    "        bin_a &= self.bitmask[self.dtype]\n",
    "        bin_b &= self.bitmask[self.dtype]\n",
    "        if bin_a >> self.dtype.mantissa_bits() == 0:\n",
    "            return 0\n",
    "        if bin_b >> self.dtype.mantissa_bits() == 0:\n",
    "            return 0\n",
    "        binint = (bin_a + bin_b + self.lmul_offset[self.dtype]) & self.bitmask[\n",
    "            self.dtype\n",
    "        ]\n",
    "        binint |= sign << (self.dtype.bitwidth() - 1)\n",
    "        return float(self.dtype(binint=binint))\n",
    "\n",
    "\n",
    "def run_lmul(a: float, b: float, dtype: Type[BaseFloat] = BF16, fast: bool = True):\n",
    "    # if fast:\n",
    "    #     lmul_fn = lmul_fast\n",
    "    # else:\n",
    "    #     lmul_fn = lmul_simple\n",
    "    lmul_fn = lmul_fix\n",
    "\n",
    "    bits = dtype.bitwidth()\n",
    "\n",
    "    reset_working_block()\n",
    "    input_a, input_b = Input(bits, \"input_a\"), Input(bits, \"input_b\")\n",
    "    output = Output(bits, \"output\")\n",
    "    output <<= lmul_fn(input_a, input_b, dtype)\n",
    "\n",
    "    tracer = SimulationTrace(\"all\")\n",
    "    sim = Simulation(tracer=tracer)\n",
    "    sim.step({input_a: dtype(a).binint, input_b: dtype(b).binint})\n",
    "\n",
    "    binary_result = sim.inspect(output)\n",
    "    result = dtype(binint=binary_result)\n",
    "\n",
    "    print(f\"{a} * {b} = {result}\")\n",
    "    print(format(binary_result, f\"0{bits}b\"))\n",
    "\n",
    "    return float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmulcls = FastLmul(BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90 µs, sys: 6 µs, total: 96 µs\n",
      "Wall time: 94.9 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lmulcls.run(0,0.000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized numpy lmul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Type, Tuple, Any\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16, Float32\n",
    "\n",
    "\n",
    "class OptimizedLmul:\n",
    "    \"\"\"Optimized implementation of LMUL using pure integer arithmetic\"\"\"\n",
    "\n",
    "    def __init__(self, dtype: Type[BaseFloat]):\n",
    "        self.dtype = dtype\n",
    "        self.lmul_offset = {\n",
    "            BF16: get_combined_offset(8, 7, True),\n",
    "            Float32: get_combined_offset(8, 23, True),\n",
    "        }\n",
    "        self.bitmask = {\n",
    "            BF16: 0b0111111111111111,\n",
    "            Float32: 0b01111111111111111111111111111111,\n",
    "        }\n",
    "        self.bitwidth = dtype.bitwidth()\n",
    "        self.mantissa_bits = dtype.mantissa_bits()\n",
    "\n",
    "    def multiply(self, bin_a: int, bin_b: int) -> int:\n",
    "        \"\"\"Multiply two numbers in binint representation using LMUL algorithm\"\"\"\n",
    "        # Extract sign bit\n",
    "        sign = (bin_a >> (self.bitwidth - 1)) ^ (bin_b >> (self.bitwidth - 1))\n",
    "\n",
    "        # Clear sign bits\n",
    "        bin_a &= self.bitmask[self.dtype]\n",
    "        bin_b &= self.bitmask[self.dtype]\n",
    "\n",
    "        # Check for zero exponents (denormals or zero)\n",
    "        if bin_a >> self.mantissa_bits == 0 or bin_b >> self.mantissa_bits == 0:\n",
    "            return 0\n",
    "\n",
    "        # Apply LMUL algorithm\n",
    "        binint = (bin_a + bin_b + self.lmul_offset[self.dtype]) & self.bitmask[\n",
    "            self.dtype\n",
    "        ]\n",
    "\n",
    "        # Set sign bit\n",
    "        binint |= sign << (self.bitwidth - 1)\n",
    "\n",
    "        return binint\n",
    "\n",
    "\n",
    "def convert_to_binint(data: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"Convert numpy array of floats to array of binint representations\"\"\"\n",
    "    # Create a vectorized function to convert each element\n",
    "    vectorized_convert = np.vectorize(lambda x: dtype(x).binint)\n",
    "    return vectorized_convert(data)\n",
    "\n",
    "\n",
    "def convert_from_binint(data: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"Convert numpy array of binint representations back to floats\"\"\"\n",
    "    # Create a vectorized function to convert each element\n",
    "    vectorized_convert = np.vectorize(lambda x: float(dtype(binint=x)))\n",
    "    return vectorized_convert(data)\n",
    "\n",
    "\n",
    "def relu_binint(x: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"Apply ReLU to binint values by checking sign bit\"\"\"\n",
    "    # For floating point in binint representation, negative numbers have the highest bit set\n",
    "    sign_mask = 1 << (dtype.bitwidth() - 1)\n",
    "    return np.where((x & sign_mask) == 0, x, 0)\n",
    "\n",
    "\n",
    "def optimized_mlp_inference(\n",
    "    inputs_batch: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Optimized MLP inference using integer arithmetic for batched inputs\n",
    "\n",
    "    Args:\n",
    "        inputs_batch: Batch of input vectors [batch_size, input_dim]\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predicted_classes, output_probabilities)\n",
    "    \"\"\"\n",
    "    # Initialize the optimized LMUL\n",
    "    lmul = OptimizedLmul(dtype)\n",
    "\n",
    "    # Pre-convert all weights and biases to binint representation\n",
    "    fc1_weight_bin = convert_to_binint(model_weights[\"fc1_weight\"], dtype)\n",
    "    fc1_bias_bin = convert_to_binint(model_weights[\"fc1_bias\"], dtype)\n",
    "    fc2_weight_bin = convert_to_binint(model_weights[\"fc2_weight\"], dtype)\n",
    "    fc2_bias_bin = convert_to_binint(model_weights[\"fc2_bias\"], dtype)\n",
    "\n",
    "    # Convert inputs to binint\n",
    "    inputs_bin = convert_to_binint(inputs_batch, dtype)\n",
    "\n",
    "    batch_size = inputs_batch.shape[0]\n",
    "    hidden_size = fc1_weight_bin.shape[0]\n",
    "    output_size = fc2_weight_bin.shape[0]\n",
    "    input_size = inputs_batch.shape[1]\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1_bin = np.zeros((batch_size, hidden_size), dtype=np.int64)\n",
    "\n",
    "    # Perform matrix multiplication with LMUL\n",
    "    for b in range(batch_size):\n",
    "        for i in range(hidden_size):\n",
    "            acc = 0  # Accumulate in regular float for now\n",
    "            for j in range(input_size):\n",
    "                # Multiply using optimized LMUL\n",
    "                prod = lmul.multiply(fc1_weight_bin[i, j], inputs_bin[b, j])\n",
    "\n",
    "                # Convert product back to float for accumulation\n",
    "                # In a real hardware implementation, this would be a floating-point addition\n",
    "                prod_float = float(dtype(binint=prod))\n",
    "                acc += prod_float\n",
    "\n",
    "            # Convert accumulated result back to binint\n",
    "            h1_bin[b, i] = dtype(acc).binint\n",
    "\n",
    "    # Add bias (in binint space, this is still a floating-point addition)\n",
    "    h1_with_bias_bin = np.zeros_like(h1_bin)\n",
    "    for b in range(batch_size):\n",
    "        for i in range(hidden_size):\n",
    "            # Convert to float, add, then convert back to binint\n",
    "            h1_float = float(dtype(binint=h1_bin[b, i]))\n",
    "            bias_float = float(dtype(binint=fc1_bias_bin[i]))\n",
    "            h1_with_bias_bin[b, i] = dtype(h1_float + bias_float).binint\n",
    "\n",
    "    # Apply ReLU in binint space\n",
    "    a1_bin = relu_binint(h1_with_bias_bin, dtype)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias\n",
    "    h_out_bin = np.zeros((batch_size, output_size), dtype=np.int64)\n",
    "\n",
    "    # Perform matrix multiplication with LMUL\n",
    "    for b in range(batch_size):\n",
    "        for i in range(output_size):\n",
    "            acc = 0  # Accumulate in regular float for now\n",
    "            for j in range(hidden_size):\n",
    "                # Multiply using optimized LMUL\n",
    "                prod = lmul.multiply(fc2_weight_bin[i, j], a1_bin[b, j])\n",
    "\n",
    "                # Convert product back to float for accumulation\n",
    "                prod_float = float(dtype(binint=prod))\n",
    "                acc += prod_float\n",
    "\n",
    "            # Convert accumulated result back to binint\n",
    "            h_out_bin[b, i] = dtype(acc).binint\n",
    "\n",
    "    # Add bias\n",
    "    h_out_with_bias_bin = np.zeros_like(h_out_bin)\n",
    "    for b in range(batch_size):\n",
    "        for i in range(output_size):\n",
    "            # Convert to float, add, then convert back to binint\n",
    "            h_out_float = float(dtype(binint=h_out_bin[b, i]))\n",
    "            bias_float = float(dtype(binint=fc2_bias_bin[i]))\n",
    "            h_out_with_bias_bin[b, i] = dtype(h_out_float + bias_float).binint\n",
    "\n",
    "    # Convert final layer output back to floats for softmax\n",
    "    h_out_float = convert_from_binint(h_out_with_bias_bin, dtype)\n",
    "\n",
    "    # Apply softmax (in float space)\n",
    "    output_probs = np.zeros_like(h_out_float)\n",
    "    for b in range(batch_size):\n",
    "        exp_x = np.exp(\n",
    "            h_out_float[b] - np.max(h_out_float[b])\n",
    "        )  # For numerical stability\n",
    "        output_probs[b] = exp_x / exp_x.sum()\n",
    "\n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(output_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pyinstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NumPy inference time for batch of 10: 0.0004 seconds\n",
      "Optimized inference time for batch of 10: 4.7595 seconds\n",
      "Speedup: 0.00x\n",
      "Prediction match rate: 10/10 (100.00%)\n",
      "\n",
      "Sample comparisons:\n",
      "Sample 0: NumPy predicted 7, Optimized predicted 7\n",
      "Sample 1: NumPy predicted 2, Optimized predicted 2\n",
      "Sample 2: NumPy predicted 1, Optimized predicted 1\n",
      "Sample 3: NumPy predicted 0, Optimized predicted 0\n",
      "Sample 4: NumPy predicted 4, Optimized predicted 4\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "    import time\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"models/mlp_mnist_bf16.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get batch of input data (assuming get_batch_activations() returns multiple samples)\n",
    "    batch_size = 10\n",
    "    inputs_batch, labels = get_batch(batch_size)\n",
    "\n",
    "    # Run standard NumPy inference for comparison\n",
    "    start_time = time.time()\n",
    "\n",
    "    numpy_results = []\n",
    "    for i in tqdm(range(batch_size)):\n",
    "        inputs = inputs_batch[i]\n",
    "        h1_numpy = inputs @ fc1_weight.T + fc1_bias\n",
    "        a1_numpy = np.maximum(0, h1_numpy)\n",
    "        h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "        a_out_numpy = softmax(h_out_numpy)\n",
    "        predicted_class_numpy = np.argmax(a_out_numpy)\n",
    "        numpy_results.append((predicted_class_numpy, a_out_numpy))\n",
    "\n",
    "    numpy_time = time.time() - start_time\n",
    "    numpy_predictions = np.array([r[0] for r in numpy_results])\n",
    "\n",
    "    print(\n",
    "        f\"Standard NumPy inference time for batch of {batch_size}: {numpy_time:.4f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Run optimized inference using integer arithmetic\n",
    "    start_time = time.time()\n",
    "    predicted_classes, output_probs = optimized_mlp_inference(\n",
    "        inputs_batch, model_weights, BF16\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Optimized inference time for batch of {batch_size}: {optimized_time:.4f} seconds\"\n",
    "    )\n",
    "    print(f\"Speedup: {numpy_time / optimized_time:.2f}x\")\n",
    "\n",
    "    # Compare results\n",
    "    match_count = np.sum(predicted_classes == numpy_predictions)\n",
    "    print(\n",
    "        f\"Prediction match rate: {match_count}/{batch_size} ({match_count/batch_size*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Show a few examples\n",
    "    print(\"\\nSample comparisons:\")\n",
    "    for i in range(min(5, batch_size)):\n",
    "        print(\n",
    "            f\"Sample {i}: NumPy predicted {numpy_predictions[i]}, Optimized predicted {predicted_classes[i]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NumPy inference time for batch of 128: 0.0003 seconds\n",
      "Optimized vectorized inference time for batch of 128: 0.7112 seconds\n",
      "Speedup: 0.00x\n",
      "Prediction match rate: 3/128 (2.34%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from typing import Type, Tuple, Dict, Any\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16, Float32\n",
    "\n",
    "\n",
    "def float_to_binint_batch(values: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized conversion of float values to binint representation\n",
    "\n",
    "    Args:\n",
    "        values: NumPy array of float values\n",
    "        dtype: Target floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # For BF16, we need to convert through float32 first\n",
    "        # Get binary representation of float32 values\n",
    "        float32_bits = np.frombuffer(\n",
    "            struct.pack(\"!%df\" % len(values.flatten()), *values.flatten()),\n",
    "            dtype=np.uint32,\n",
    "        ).reshape(values.shape)\n",
    "\n",
    "        # Extract parts from float32 and construct BF16\n",
    "        sign = (float32_bits >> 31) & 0x1\n",
    "        exp = (float32_bits >> 23) & 0xFF\n",
    "        mantissa = (float32_bits >> 16) & 0x7F  # Keep only top 7 bits of mantissa\n",
    "\n",
    "        # Combine into BF16 binint (16 bits)\n",
    "        return ((sign << 15) | (exp << 7) | mantissa).astype(np.uint16)\n",
    "\n",
    "    elif dtype == Float32:\n",
    "        # For Float32, we can directly convert\n",
    "        return np.frombuffer(\n",
    "            struct.pack(\"!%df\" % len(values.flatten()), *values.flatten()),\n",
    "            dtype=np.uint32,\n",
    "        ).reshape(values.shape)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "def binint_to_float_batch(binints: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized conversion of binint representations back to float values\n",
    "\n",
    "    Args:\n",
    "        binints: NumPy array of binint representations\n",
    "        dtype: Source floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of float values\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # Extract parts from BF16\n",
    "        sign = (binints >> 15) & 0x1\n",
    "        exp = (binints >> 7) & 0xFF\n",
    "        mantissa = binints & 0x7F\n",
    "\n",
    "        # Construct float32 representation\n",
    "        float32_bits = (sign << 31) | (exp << 23) | (mantissa << 16)\n",
    "\n",
    "        # Convert to float32\n",
    "        return np.frombuffer(\n",
    "            struct.pack(\"!%dI\" % len(float32_bits.flatten()), *float32_bits.flatten()),\n",
    "            dtype=np.float32,\n",
    "        ).reshape(binints.shape)\n",
    "\n",
    "    elif dtype == Float32:\n",
    "        # For Float32, we can directly convert\n",
    "        return np.frombuffer(\n",
    "            struct.pack(\"!%dI\" % len(binints.flatten()), *binints.flatten()),\n",
    "            dtype=np.float32,\n",
    "        ).reshape(binints.shape)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "def lmul_vectorized(\n",
    "    a_binint: np.ndarray, b_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized implementation of LMUL algorithm\n",
    "\n",
    "    Args:\n",
    "        a_binint: NumPy array of binint representations\n",
    "        b_binint: NumPy array of binint representations\n",
    "        dtype: Floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations of results\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # Constants for BF16\n",
    "        bitmask = 0x7FFF  # 15 bits (excluding sign)\n",
    "        bitwidth = 16\n",
    "        mantissa_bits = 7\n",
    "        lmul_offset = get_combined_offset(8, 7, True)\n",
    "    elif dtype == Float32:\n",
    "        # Constants for Float32\n",
    "        bitmask = 0x7FFFFFFF  # 31 bits (excluding sign)\n",
    "        bitwidth = 32\n",
    "        mantissa_bits = 23\n",
    "        lmul_offset = get_combined_offset(8, 23, True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "    # Extract sign bits\n",
    "    sign_a = (a_binint >> (bitwidth - 1)) & 0x1\n",
    "    sign_b = (b_binint >> (bitwidth - 1)) & 0x1\n",
    "\n",
    "    # Compute result sign\n",
    "    sign_result = sign_a ^ sign_b\n",
    "\n",
    "    # Clear sign bits\n",
    "    a_unsigned = a_binint & bitmask\n",
    "    b_unsigned = b_binint & bitmask\n",
    "\n",
    "    # Create masks for zero exponents\n",
    "    a_exp = a_unsigned >> mantissa_bits\n",
    "    b_exp = b_unsigned >> mantissa_bits\n",
    "\n",
    "    # Apply LMUL algorithm where both exponents are non-zero\n",
    "    result = np.zeros_like(a_binint)\n",
    "    valid_mask = (a_exp != 0) & (b_exp != 0)\n",
    "\n",
    "    if np.any(valid_mask):\n",
    "        # Only compute for valid inputs\n",
    "        result[valid_mask] = (\n",
    "            a_unsigned[valid_mask] + b_unsigned[valid_mask] + lmul_offset\n",
    "        ) & bitmask\n",
    "\n",
    "        # Set sign bits\n",
    "        result[valid_mask] |= sign_result[valid_mask] << (bitwidth - 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def matrix_vector_multiply_lmul(\n",
    "    weights_binint: np.ndarray, inputs_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Optimized matrix-vector multiplication using vectorized LMUL\n",
    "\n",
    "    Args:\n",
    "        weights_binint: Weight matrix in binint representation [output_dim, input_dim]\n",
    "        inputs_binint: Input vector in binint representation [batch_size, input_dim]\n",
    "        dtype: Floating-point type\n",
    "\n",
    "    Returns:\n",
    "        Result in float representation [batch_size, output_dim]\n",
    "    \"\"\"\n",
    "    batch_size, input_dim = inputs_binint.shape\n",
    "    output_dim = weights_binint.shape[0]\n",
    "\n",
    "    # Reshape inputs for broadcasting\n",
    "    inputs_reshaped = inputs_binint.reshape(batch_size, 1, input_dim)\n",
    "\n",
    "    # Broadcast weights for batch processing\n",
    "    weights_broadcast = np.broadcast_to(\n",
    "        weights_binint.reshape(1, output_dim, input_dim),\n",
    "        (batch_size, output_dim, input_dim),\n",
    "    )\n",
    "\n",
    "    # Apply LMUL to all pairs of weights and inputs\n",
    "    products_binint = lmul_vectorized(\n",
    "        weights_broadcast.reshape(-1),\n",
    "        np.broadcast_to(inputs_reshaped, (batch_size, output_dim, input_dim)).reshape(\n",
    "            -1\n",
    "        ),\n",
    "        dtype,\n",
    "    ).reshape(batch_size, output_dim, input_dim)\n",
    "\n",
    "    # Convert products to float for summation\n",
    "    products_float = binint_to_float_batch(products_binint, dtype)\n",
    "\n",
    "    # Sum along input dimension\n",
    "    result_float = np.sum(products_float, axis=2)\n",
    "\n",
    "    return result_float\n",
    "\n",
    "\n",
    "def add_bias_vectorized(activations: np.ndarray, bias: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Add bias to activations (in float space)\"\"\"\n",
    "    return activations + bias\n",
    "\n",
    "\n",
    "def relu_vectorized(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply ReLU activation\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax_vectorized(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply softmax activation\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def optimized_mlp_inference_vectorized(\n",
    "    inputs_batch: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Highly optimized MLP inference using vectorized operations\n",
    "\n",
    "    Args:\n",
    "        inputs_batch: Batch of input vectors [batch_size, input_dim]\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predicted_classes, output_probabilities)\n",
    "    \"\"\"\n",
    "    # Pre-convert all weights and biases to binint representation\n",
    "    fc1_weight_bin = float_to_binint_batch(model_weights[\"fc1_weight\"], dtype)\n",
    "    fc1_bias = model_weights[\"fc1_bias\"]  # Keep biases as float for addition\n",
    "    fc2_weight_bin = float_to_binint_batch(model_weights[\"fc2_weight\"], dtype)\n",
    "    fc2_bias = model_weights[\"fc2_bias\"]  # Keep biases as float for addition\n",
    "\n",
    "    # Convert inputs to binint\n",
    "    inputs_bin = float_to_binint_batch(inputs_batch, dtype)\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1_float = matrix_vector_multiply_lmul(fc1_weight_bin, inputs_bin, dtype)\n",
    "    h1_with_bias = add_bias_vectorized(h1_float, fc1_bias)\n",
    "    a1 = relu_vectorized(h1_with_bias)\n",
    "\n",
    "    # Convert activations back to binint for next layer\n",
    "    a1_bin = float_to_binint_batch(a1, dtype)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias + softmax\n",
    "    h_out_float = matrix_vector_multiply_lmul(fc2_weight_bin, a1_bin, dtype)\n",
    "    h_out_with_bias = add_bias_vectorized(h_out_float, fc2_bias)\n",
    "\n",
    "    # Apply softmax\n",
    "    output_probs = softmax_vectorized(h_out_with_bias)\n",
    "\n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(output_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, output_probs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "    import time\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get batch of input data\n",
    "    batch_size = 128\n",
    "    inputs_batch, labels = get_batch(batch_size)\n",
    "\n",
    "    # Run standard NumPy inference for comparison\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Vectorized standard NumPy implementation\n",
    "    h1_numpy = inputs_batch @ fc1_weight.T + fc1_bias\n",
    "    a1_numpy = np.maximum(0, h1_numpy)\n",
    "    h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "    exp_x = np.exp(h_out_numpy - np.max(h_out_numpy, axis=1, keepdims=True))\n",
    "    a_out_numpy = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    numpy_predictions = np.argmax(a_out_numpy, axis=1)\n",
    "\n",
    "    numpy_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"Standard NumPy inference time for batch of {batch_size}: {numpy_time:.4f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Run optimized inference using vectorized operations\n",
    "    start_time = time.time()\n",
    "    predicted_classes, output_probs = optimized_mlp_inference_vectorized(\n",
    "        inputs_batch, model_weights, BF16\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Optimized vectorized inference time for batch of {batch_size}: {optimized_time:.4f} seconds\"\n",
    "    )\n",
    "    print(f\"Speedup: {numpy_time / optimized_time:.2f}x\")\n",
    "\n",
    "    # Compare results\n",
    "    match_count = np.sum(predicted_classes == numpy_predictions)\n",
    "    print(\n",
    "        f\"Prediction match rate: {match_count}/{batch_size} ({match_count/batch_size*100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0625, 4.25, 8.5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hardware_accelerators.simulation.matrix_utils import convert_array_dtype\n",
    "\n",
    "bin_array = convert_array_dtype(np.array([0, 1, 2, 3]), BF16)\n",
    "\n",
    "[BF16(binint=x).decimal_approx for x in lmul_vectorized(bin_array, bin_array, BF16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 8) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 236\u001b[0m\n\u001b[1;32m    233\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Vectorized standard NumPy implementation\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m h1_numpy \u001b[38;5;241m=\u001b[39m \u001b[43minputs_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfc1_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m \u001b[38;5;241m+\u001b[39m fc1_bias\n\u001b[1;32m    237\u001b[0m a1_numpy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, h1_numpy)\n\u001b[1;32m    238\u001b[0m h_out_numpy \u001b[38;5;241m=\u001b[39m a1_numpy \u001b[38;5;241m@\u001b[39m fc2_weight\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m fc2_bias\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 8) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Type, Tuple, Dict, Any\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16, Float32\n",
    "\n",
    "\n",
    "def convert_array_dtype(values: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert array of float values to array of binint representations using the dtype class\n",
    "\n",
    "    Args:\n",
    "        values: NumPy array of float values\n",
    "        dtype: Target floating-point type\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations\n",
    "    \"\"\"\n",
    "    # Flatten the array for processing\n",
    "    original_shape = values.shape\n",
    "    flat_values = values.flatten()\n",
    "\n",
    "    # Create array to hold results\n",
    "    result = np.zeros(\n",
    "        flat_values.shape, dtype=np.uint32 if dtype.bitwidth() == 32 else np.uint16\n",
    "    )\n",
    "\n",
    "    # Convert each value using the dtype class\n",
    "    for i, val in enumerate(flat_values):\n",
    "        result[i] = dtype(val).binint\n",
    "\n",
    "    # Reshape back to original shape\n",
    "    return result.reshape(original_shape)\n",
    "\n",
    "\n",
    "def convert_binint_to_float(binints: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert array of binint representations back to float values using the dtype class\n",
    "\n",
    "    Args:\n",
    "        binints: NumPy array of binint representations\n",
    "        dtype: Source floating-point type\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of float values\n",
    "    \"\"\"\n",
    "    # Flatten the array for processing\n",
    "    original_shape = binints.shape\n",
    "    flat_binints = binints.flatten()\n",
    "\n",
    "    # Create array to hold results\n",
    "    result = np.zeros(flat_binints.shape, dtype=np.float32)\n",
    "\n",
    "    # Convert each binint using the dtype class\n",
    "    for i, binint in enumerate(flat_binints):\n",
    "        result[i] = float(dtype(binint=int(binint)))\n",
    "\n",
    "    # Reshape back to original shape\n",
    "    return result.reshape(original_shape)\n",
    "\n",
    "\n",
    "def lmul_vectorized(\n",
    "    a_binint: np.ndarray, b_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized implementation of LMUL algorithm\n",
    "\n",
    "    Args:\n",
    "        a_binint: NumPy array of binint representations\n",
    "        b_binint: NumPy array of binint representations\n",
    "        dtype: Floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations of results\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # Constants for BF16\n",
    "        bitmask = 0x7FFF  # 15 bits (excluding sign)\n",
    "        bitwidth = 16\n",
    "        mantissa_bits = 7\n",
    "        lmul_offset = get_combined_offset(8, 7, True)\n",
    "    elif dtype == Float32:\n",
    "        # Constants for Float32\n",
    "        bitmask = 0x7FFFFFFF  # 31 bits (excluding sign)\n",
    "        bitwidth = 32\n",
    "        mantissa_bits = 23\n",
    "        lmul_offset = get_combined_offset(8, 23, True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "    # Create output array\n",
    "    result = np.zeros_like(a_binint)\n",
    "\n",
    "    # Flatten arrays for processing\n",
    "    a_flat = a_binint.flatten()\n",
    "    b_flat = b_binint.flatten()\n",
    "    result_flat = result.flatten()\n",
    "\n",
    "    # Process each pair of values\n",
    "    for i in range(len(a_flat)):\n",
    "        a_val = int(a_flat[i])\n",
    "        b_val = int(b_flat[i])\n",
    "\n",
    "        # Extract sign bits\n",
    "        sign_a = (a_val >> (bitwidth - 1)) & 0x1\n",
    "        sign_b = (b_val >> (bitwidth - 1)) & 0x1\n",
    "        sign_result = sign_a ^ sign_b\n",
    "\n",
    "        # Clear sign bits\n",
    "        a_unsigned = a_val & bitmask\n",
    "        b_unsigned = b_val & bitmask\n",
    "\n",
    "        # Check for zero exponents\n",
    "        a_exp = a_unsigned >> mantissa_bits\n",
    "        b_exp = b_unsigned >> mantissa_bits\n",
    "\n",
    "        if a_exp == 0 or b_exp == 0:\n",
    "            result_flat[i] = 0\n",
    "        else:\n",
    "            # Apply LMUL algorithm\n",
    "            result_val = (a_unsigned + b_unsigned + lmul_offset) & bitmask\n",
    "            result_val |= sign_result << (bitwidth - 1)\n",
    "            result_flat[i] = result_val\n",
    "\n",
    "    return result.reshape(a_binint.shape)\n",
    "\n",
    "\n",
    "def batch_matrix_vector_multiply(\n",
    "    weights_binint: np.ndarray, inputs_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform matrix-vector multiplication for a batch of inputs using LMUL\n",
    "\n",
    "    Args:\n",
    "        weights_binint: Weight matrix in binint representation [output_dim, input_dim]\n",
    "        inputs_binint: Input vectors in binint representation [batch_size, input_dim]\n",
    "        dtype: Floating-point type\n",
    "\n",
    "    Returns:\n",
    "        Result in float representation [batch_size, output_dim]\n",
    "    \"\"\"\n",
    "    batch_size, input_dim = inputs_binint.shape\n",
    "    output_dim, input_dim_w = weights_binint.shape\n",
    "\n",
    "    if input_dim != input_dim_w:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch: inputs {input_dim} vs weights {input_dim_w}\"\n",
    "        )\n",
    "\n",
    "    # Initialize result array\n",
    "    result_float = np.zeros((batch_size, output_dim), dtype=np.float32)\n",
    "\n",
    "    # Process each batch item and output neuron\n",
    "    for b in range(batch_size):\n",
    "        for o in range(output_dim):\n",
    "            # Multiply weights with inputs using LMUL\n",
    "            products_binint = lmul_vectorized(\n",
    "                np.broadcast_to(weights_binint[o], input_dim), inputs_binint[b], dtype\n",
    "            )\n",
    "\n",
    "            # Convert products to float for summation\n",
    "            products_float = convert_binint_to_float(products_binint, dtype)\n",
    "\n",
    "            # Sum the products\n",
    "            result_float[b, o] = np.sum(products_float)\n",
    "\n",
    "    return result_float\n",
    "\n",
    "\n",
    "def optimized_mlp_inference(\n",
    "    inputs_batch: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Optimized MLP inference using LMUL for matrix multiplications\n",
    "\n",
    "    Args:\n",
    "        inputs_batch: Batch of input vectors [batch_size, input_dim]\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predicted_classes, output_probabilities)\n",
    "    \"\"\"\n",
    "    # Pre-convert all weights to binint representation\n",
    "    fc1_weight_bin = convert_array_dtype(model_weights[\"fc1_weight\"], dtype)\n",
    "    fc1_bias = model_weights[\"fc1_bias\"]  # Keep biases as float for addition\n",
    "    fc2_weight_bin = convert_array_dtype(model_weights[\"fc2_weight\"], dtype)\n",
    "    fc2_bias = model_weights[\"fc2_bias\"]  # Keep biases as float for addition\n",
    "\n",
    "    # Convert inputs to binint\n",
    "    inputs_bin = convert_array_dtype(inputs_batch, dtype)\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1_float = batch_matrix_vector_multiply(fc1_weight_bin, inputs_bin, dtype)\n",
    "    h1_with_bias = h1_float + fc1_bias  # Add bias\n",
    "    a1 = np.maximum(0, h1_with_bias)  # ReLU\n",
    "\n",
    "    # Convert activations back to binint for next layer\n",
    "    a1_bin = convert_array_dtype(a1, dtype)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias\n",
    "    h_out_float = batch_matrix_vector_multiply(fc2_weight_bin, a1_bin, dtype)\n",
    "    h_out_with_bias = h_out_float + fc2_bias  # Add bias\n",
    "\n",
    "    # Apply softmax\n",
    "    exp_x = np.exp(h_out_with_bias - np.max(h_out_with_bias, axis=1, keepdims=True))\n",
    "    output_probs = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(output_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, output_probs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "    import time\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get batch of input data\n",
    "    batch_size = 8  # Smaller batch for testing\n",
    "    inputs_batch = get_batch(batch_size)\n",
    "\n",
    "    # Run standard NumPy inference for comparison\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Vectorized standard NumPy implementation\n",
    "    h1_numpy = inputs_batch @ fc1_weight.T + fc1_bias\n",
    "    a1_numpy = np.maximum(0, h1_numpy)\n",
    "    h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "    exp_x = np.exp(h_out_numpy - np.max(h_out_numpy, axis=1, keepdims=True))\n",
    "    a_out_numpy = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    numpy_predictions = np.argmax(a_out_numpy, axis=1)\n",
    "\n",
    "    numpy_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"Standard NumPy inference time for batch of {batch_size}: {numpy_time:.4f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Run optimized inference\n",
    "    start_time = time.time()\n",
    "    predicted_classes, output_probs = optimized_mlp_inference(\n",
    "        inputs_batch, model_weights, BF16\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Optimized inference time for batch of {batch_size}: {optimized_time:.4f} seconds\"\n",
    "    )\n",
    "    print(f\"Speedup: {numpy_time / optimized_time:.2f}x\")\n",
    "\n",
    "    # Compare results\n",
    "    match_count = np.sum(predicted_classes == numpy_predictions)\n",
    "    print(\n",
    "        f\"Prediction match rate: {match_count}/{batch_size} ({match_count/batch_size*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Print detailed comparison for first few examples\n",
    "    print(\"\\nDetailed comparison for first 5 examples:\")\n",
    "    for i in range(min(5, batch_size)):\n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"  NumPy prediction: {numpy_predictions[i]}\")\n",
    "        print(f\"  Optimized prediction: {predicted_classes[i]}\")\n",
    "        print(f\"  Top NumPy probabilities: {np.sort(a_out_numpy[i])[-3:]}\")\n",
    "        print(f\"  Top Optimized probabilities: {np.sort(output_probs[i])[-3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = pyinstrument.Profiler(interval=0.000001)\n",
    "\n",
    "profiler.start()\n",
    "lmulcls(0, 0)\n",
    "profiler.stop()\n",
    "print(profiler.output_text(unicode=True, color=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "0.8 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 * 4 = 0.4140625\n",
      "0011111011010100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4140625"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_lmul(0.1, 4, fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NumPy Inference - Predicted class: 7\n",
      "FastLmul Inference (BF16) - Predicted class: 7\n",
      "\n",
      "Output Probabilities Comparison:\n",
      "Class 0: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 1: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 2: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 3: NumPy = 0.000004, FastLmul = 0.000005, Diff = 0.000001\n",
      "Class 4: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 5: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 6: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 7: NumPy = 0.999996, FastLmul = 0.999994, Diff = 0.000001\n",
      "Class 8: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 9: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "\n",
      "Both methods predicted the same class! ✓\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Type\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16\n",
    "\n",
    "\n",
    "def matrix_vector_multiply_with_lmul(\n",
    "    A: np.ndarray, v: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform matrix-vector multiplication using FastLmul for individual multiplications\n",
    "\n",
    "    Args:\n",
    "        A: Matrix as numpy array\n",
    "        v: Vector as numpy array\n",
    "        dtype: The floating-point data type to use with FastLmul\n",
    "\n",
    "    Returns:\n",
    "        Result vector as numpy array\n",
    "    \"\"\"\n",
    "    # Initialize the FastLmul instance\n",
    "    lmul = FastLmul(dtype)\n",
    "\n",
    "    # Get dimensions\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Verify that the matrix and vector can be multiplied\n",
    "    if v.shape[0] != n:\n",
    "        raise ValueError(\n",
    "            f\"Dimensions don't match for multiplication: A is {m}x{n}, v is {v.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    # Initialize result vector with zeros\n",
    "    result = np.zeros(m)\n",
    "\n",
    "    # Perform matrix-vector multiplication\n",
    "    for i in range(m):\n",
    "        sum_val = 0.0\n",
    "        for j in range(n):\n",
    "            # Use FastLmul's run method instead of regular multiplication\n",
    "            product = lmul(A[i, j], v[j])\n",
    "            # product = lmul.run(A[i, j], v[j])\n",
    "            sum_val += product\n",
    "        result[i] = sum_val\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def vector_add(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Simple vector addition without using lmul\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "\n",
    "def simulate_mlp_inference_with_lmul(\n",
    "    input_data: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Simulate MLP inference using FastLmul for matrix multiplications\n",
    "\n",
    "    Args:\n",
    "        input_data: Input vector\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use with FastLmul\n",
    "\n",
    "    Returns:\n",
    "        Predicted class index\n",
    "    \"\"\"\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model_weights[\"fc1_weight\"]\n",
    "    fc1_bias = model_weights[\"fc1_bias\"]\n",
    "    fc2_weight = model_weights[\"fc2_weight\"]\n",
    "    fc2_bias = model_weights[\"fc2_bias\"]\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1 = matrix_vector_multiply_with_lmul(fc1_weight, input_data, dtype)\n",
    "    h1 = vector_add(h1, fc1_bias)\n",
    "    a1 = relu(h1)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias + softmax\n",
    "    h_out = matrix_vector_multiply_with_lmul(fc2_weight, a1, dtype)\n",
    "    h_out = vector_add(h_out, fc2_bias)\n",
    "    a_out = softmax(h_out)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(a_out)\n",
    "\n",
    "    return predicted_class, a_out\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get input data (assuming get_activation() returns a sample from MNIST)\n",
    "    inputs = get_activation()\n",
    "\n",
    "    # Run inference using standard numpy operations\n",
    "    h1_numpy = inputs @ fc1_weight.T + fc1_bias\n",
    "    a1_numpy = np.maximum(0, h1_numpy)\n",
    "    h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "    a_out_numpy = softmax(h_out_numpy)\n",
    "    predicted_class_numpy = np.argmax(a_out_numpy)\n",
    "\n",
    "    print(f\"Standard NumPy Inference - Predicted class: {predicted_class_numpy}\")\n",
    "\n",
    "    # Run inference using FastLmul\n",
    "    predicted_class_lmul, a_out_lmul = simulate_mlp_inference_with_lmul(\n",
    "        inputs, model_weights, BF16\n",
    "    )\n",
    "\n",
    "    print(f\"FastLmul Inference (BF16) - Predicted class: {predicted_class_lmul}\")\n",
    "\n",
    "    # Compare the output probabilities\n",
    "    print(\"\\nOutput Probabilities Comparison:\")\n",
    "    for i in range(len(a_out_numpy)):\n",
    "        print(\n",
    "            f\"Class {i}: NumPy = {a_out_numpy[i]:.6f}, FastLmul = {a_out_lmul[i]:.6f}, \"\n",
    "            f\"Diff = {abs(a_out_numpy[i] - a_out_lmul[i]):.6f}\"\n",
    "        )\n",
    "\n",
    "    # Check if the predictions match\n",
    "    if predicted_class_numpy == predicted_class_lmul:\n",
    "        print(\"\\nBoth methods predicted the same class! ✓\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nPrediction mismatch: NumPy predicted {predicted_class_numpy}, \"\n",
    "            f\"FastLmul predicted {predicted_class_lmul} ✗\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "systolic_sim = SystolicArraySimulator(8, multiplier=lmul_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00762939, -0.02355957,  0.04174805, ...,  0.01428223,\n",
       "        -0.02355957,  0.0039978 ],\n",
       "       [ 0.03027344,  0.00610352,  0.04248047, ..., -0.00167847,\n",
       "         0.03173828,  0.03198242],\n",
       "       [-0.00056458,  0.01794434,  0.00424194, ...,  0.0279541 ,\n",
       "         0.01501465,  0.00738525],\n",
       "       ...,\n",
       "       [ 0.00288391, -0.02893066,  0.01831055, ..., -0.02929688,\n",
       "         0.01165771,  0.01397705],\n",
       "       [ 0.02734375, -0.01879883,  0.02734375, ...,  0.0246582 ,\n",
       "         0.02062988,  0.02722168],\n",
       "       [ 0.00787354, -0.00135803,  0.0201416 , ...,  0.02124023,\n",
       "        -0.03344727, -0.00189972]], shape=(128, 784), dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"models/mlp_mnist_bf16.pth\")\n",
    "weights = model.fc1.weight.data.numpy(force=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n",
       "        0.15599452, 0.05808361, 0.86617615],\n",
       "       [0.60111501, 0.70807258, 0.02058449, 0.96990985, 0.83244264,\n",
       "        0.21233911, 0.18182497, 0.18340451],\n",
       "       [0.30424224, 0.52475643, 0.43194502, 0.29122914, 0.61185289,\n",
       "        0.13949386, 0.29214465, 0.36636184],\n",
       "       [0.45606998, 0.78517596, 0.19967378, 0.51423444, 0.59241457,\n",
       "        0.04645041, 0.60754485, 0.17052412],\n",
       "       [0.06505159, 0.94888554, 0.96563203, 0.80839735, 0.30461377,\n",
       "        0.09767211, 0.68423303, 0.44015249],\n",
       "       [0.12203823, 0.49517691, 0.03438852, 0.9093204 , 0.25877998,\n",
       "        0.66252228, 0.31171108, 0.52006802],\n",
       "       [0.54671028, 0.18485446, 0.96958463, 0.77513282, 0.93949894,\n",
       "        0.89482735, 0.59789998, 0.92187424],\n",
       "       [0.0884925 , 0.19598286, 0.04522729, 0.32533033, 0.38867729,\n",
       "        0.27134903, 0.82873751, 0.35675333]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "fake_acts = np.random.rand(8, 8)\n",
    "fake_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_acts2 = la1.copy().reshape(8, -1)[:8, :8]\n",
    "\n",
    "gt = weights[:8, :8] @ fake_acts2\n",
    "sim_result = SystolicArraySimulator.matrix_multiply(\n",
    "    weights[:8, :8], fake_acts, multiplier=lmul_fast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.034  0.21   0.02   0.387 -0.132  0.245 -0.062 -0.159]\n",
      " [ 0.063  0.223  0.325  0.617  0.272  1.126  0.309  0.252]\n",
      " [-0.011 -0.018 -0.057 -0.392 -0.106 -0.305 -0.002  0.036]\n",
      " [ 0.009 -0.101 -0.209 -0.316 -0.246 -0.767 -0.445 -0.234]\n",
      " [ 0.009  0.081 -0.042  0.041 -0.183 -0.232 -0.001  0.443]\n",
      " [-0.056  0.039 -0.207 -0.223 -0.31  -0.809  0.019 -0.014]\n",
      " [ 0.008  0.105  0.265  0.17   0.418  0.516  0.368  0.396]\n",
      " [-0.062 -0.003 -0.212 -0.596 -0.282 -0.794 -0.058 -0.248]] \n",
      "\n",
      "[[ 0.013 -0.004  0.013 -0.014  0.03   0.009  0.027  0.013]\n",
      " [ 0.061  0.138  0.093  0.129  0.1    0.056  0.102  0.102]\n",
      " [-0.008 -0.007 -0.015 -0.014 -0.014 -0.019 -0.023 -0.02 ]\n",
      " [-0.068 -0.16  -0.068 -0.143 -0.09  -0.03  -0.06  -0.078]\n",
      " [-0.006  0.019  0.071  0.013  0.008  0.009  0.032  0.028]\n",
      " [-0.016 -0.048  0.004 -0.05  -0.029 -0.019 -0.047 -0.02 ]\n",
      " [ 0.034  0.106  0.096  0.118  0.055  0.06   0.054  0.098]\n",
      " [-0.02  -0.058 -0.056 -0.074 -0.046 -0.049 -0.081 -0.059]] \n",
      "\n",
      "[[False False  True False False False False False]\n",
      " [ True False False False False False False False]\n",
      " [ True False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False  True]\n",
      " [False  True False False False False False False]\n",
      " [False False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array2string(gt, precision=3, suppress_small=True), \"\\n\")\n",
    "print(np.array2string(sim_result, precision=3, suppress_small=True), \"\\n\")\n",
    "print(np.isclose(gt, sim_result, atol=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulating lmul with pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9436798631220628e-38, 0.0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, B = 3.182, 0\n",
    "\n",
    "# dtype = torch.bfloat16\n",
    "dtype = torch.float32\n",
    "\n",
    "\n",
    "def lmul(a, b, dtype):\n",
    "    torch_viewmap = {\n",
    "        torch.float32: torch.uint32,\n",
    "        torch.bfloat16: torch.uint16,\n",
    "        torch.float16: torch.uint16,\n",
    "    }\n",
    "\n",
    "    lmul_offset = {\n",
    "        torch.bfloat16: 16248,\n",
    "        torch.float32: 1064828928,\n",
    "    }\n",
    "\n",
    "    a = torch.tensor(a, dtype=dtype).view(torch_viewmap[dtype]).item()\n",
    "    b = torch.tensor(b, dtype=dtype).view(torch_viewmap[dtype]).item()\n",
    "\n",
    "    lmul_ab = a + b - lmul_offset[dtype]\n",
    "    # print(format(lmul_ab, f'0{torch_viewmap[dtype].itemsize * 8}b'))\n",
    "    lmul_ab = torch.tensor(lmul_ab, dtype=torch_viewmap[dtype]).view(dtype).item()\n",
    "\n",
    "    return lmul_ab\n",
    "\n",
    "\n",
    "lmul(A, B, dtype), A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064828928"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combined_offset(8, 23, twos_comp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self.dim() cannot be 0 to view Float as UInt16 (different element sizes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_viewmap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self.dim() cannot be 0 to view Float as UInt16 (different element sizes)"
     ]
    }
   ],
   "source": [
    "torch.scalar_tensor(A).view(torch_viewmap[dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate product:\n",
      " tensor([[-0.3559, -0.0709, -0.7935,  1.2156],\n",
      "        [-0.0933,  0.4330, -0.1775,  1.1919]])\n",
      "Exact product:\n",
      " tensor([[-0.3431, -0.0402, -0.8274,  1.1742],\n",
      "        [-0.0883,  0.4810, -0.1755,  1.1607]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def lmul_matmul(A: torch.Tensor, B: torch.Tensor, dtype=torch.float32):\n",
    "    if dtype == torch.float32:\n",
    "        # Use uint32 view, but cast to int64 for arithmetic\n",
    "        A_int = A.view(torch.uint32).to(torch.int64)\n",
    "        B_int = B.view(torch.uint32).to(torch.int64)\n",
    "        offset = 1064828928  # special offset for float32\n",
    "    elif dtype == torch.bfloat16:\n",
    "        A_int = A.view(torch.uint16).to(torch.int64)\n",
    "        B_int = B.view(torch.uint16).to(torch.int64)\n",
    "        offset = 16248  # special offset for bfloat16\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dtype\")\n",
    "\n",
    "    # Suppose A is (m, n) and B is (n, p)\n",
    "    # Expand dimensions so we can broadcast:\n",
    "    # A_int -> (m, n, 1) and B_int -> (1, n, p)\n",
    "    prod_int = A_int.unsqueeze(2) + B_int.unsqueeze(0) - offset  # shape: (m, n, p)\n",
    "\n",
    "    # Convert the integer results back to floating point:\n",
    "    if dtype == torch.float32:\n",
    "        prod = prod_int.to(torch.uint32).view(torch.float32)\n",
    "    else:  # bfloat16 case\n",
    "        prod = prod_int.to(torch.uint16).view(torch.bfloat16)\n",
    "\n",
    "    # Sum over the reduction dimension to complete the dot product\n",
    "    return prod.sum(dim=1)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "m, n, p = 2, 3, 4  # for instance\n",
    "A = torch.randn(m, n, dtype=torch.float32)\n",
    "B = torch.randn(n, p, dtype=torch.float32)\n",
    "C = lmul_matmul(A, B, dtype=torch.float32)\n",
    "print(\"Approximate product:\\n\", C)\n",
    "\n",
    "# Verify with exact product\n",
    "exact_product = torch.matmul(A, B)\n",
    "print(\"Exact product:\\n\", exact_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading mnist data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: convert images to tensor and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "# Download MNIST test data\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    batch, labels = next(iter(loader))\n",
    "    return batch.reshape(batch_size, -1).numpy(), labels.numpy()\n",
    "\n",
    "\n",
    "def get_activation():\n",
    "    image, _ = next(iter(test_loader))\n",
    "    image = image.detach().numpy().reshape(-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "fc1_weight = model.fc1.weight.data.numpy()\n",
    "fc1_bias = model.fc1.bias.data.numpy()\n",
    "fc2_weight = model.fc2.weight.data.numpy()\n",
    "fc2_bias = model.fc2.bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "fc1_weight = model.fc1.weight.data.numpy()\n",
    "fc1_bias = model.fc1.bias.data.numpy()\n",
    "fc2_weight = model.fc2.weight.data.numpy()\n",
    "fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "inputs = get_activation()\n",
    "\n",
    "h1 = inputs @ fc1_weight.T + fc1_bias\n",
    "a1 = np.maximum(0, h1)\n",
    "h_out = a1 @ fc2_weight.T + fc2_bias\n",
    "a_out = softmax(h_out)\n",
    "\n",
    "# get the index of the maximum value\n",
    "predicted_class = np.argmax(a_out)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 7\n"
     ]
    }
   ],
   "source": [
    "inputs = get_activation()\n",
    "\n",
    "h1 = inputs @ fc1_weight.T + fc1_bias\n",
    "a1 = np.maximum(0, h1)\n",
    "h_out = a1 @ fc2_weight.T + fc2_bias\n",
    "a_out = softmax(h_out)\n",
    "\n",
    "# get the index of the maximum value\n",
    "predicted_class = np.argmax(a_out)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lmul inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[inf, nan, inf, inf, nan, nan, inf, inf, nan, inf]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = get_batch(1)\n",
    "\n",
    "lh1 = (\n",
    "    lmul_matmul(\n",
    "        torch.tensor(inputs, dtype=torch.float32),\n",
    "        torch.tensor(fc1_weight.T, dtype=torch.float32),\n",
    "        torch.float32,\n",
    "    ).numpy()\n",
    "    + fc1_bias\n",
    ")\n",
    "la1 = np.maximum(0, lh1)\n",
    "lh_out = (\n",
    "    lmul_matmul(\n",
    "        torch.tensor(la1, dtype=torch.float32),\n",
    "        torch.tensor(fc2_weight.T, dtype=torch.float32),\n",
    "        torch.float32,\n",
    "    ).numpy()\n",
    "    + fc2_bias\n",
    ")\n",
    "# la_out = softmax(lh_out)\n",
    "\n",
    "# # get the index of the maximum value\n",
    "# predicted_class = np.argmax(la_out)\n",
    "# print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "lh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[inf, nan, inf, inf, nan, nan, inf, inf, nan, inf]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmul_matmul(\n",
    "    torch.tensor(la1, dtype=torch.float32),\n",
    "    torch.tensor(fc2_weight.T, dtype=torch.float32),\n",
    "    torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  3.1820114 ,  0.        ,  0.        ,\n",
       "         6.688619  ,  6.5706654 ,  0.        ,  0.        ,  2.4044447 ,\n",
       "         3.5749934 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.9750876 ,  0.        ,  0.        ,\n",
       "         0.        ,  4.382746  ,  7.232212  ,  0.        ,  0.        ,\n",
       "         0.        ,  2.6718044 ,  0.        ,  4.2498627 ,  0.30033502,\n",
       "         1.1679474 ,  0.        ,  1.2926131 ,  0.        ,  0.        ,\n",
       "         5.2734685 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         9.827601  ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  9.037512  ,  0.        ,  7.0829687 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.66314924,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.31634068,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  8.314439  ,  2.2188861 ,  0.892233  ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.8175012 ,\n",
       "         0.        ,  0.        ,  2.8819642 ,  2.4174857 ,  9.465485  ,\n",
       "         7.8511505 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  4.834473  , 11.010447  ,  0.        ,  0.5549504 ,\n",
       "         4.236378  ,  0.        ,  2.334083  ,  0.        , 11.079065  ,\n",
       "         0.        ,  0.        ,  3.9959395 ,  0.        ,  0.97983366,\n",
       "         7.002312  ,  0.        ,  1.5550461 ,  0.        ,  0.61436313,\n",
       "         0.        ,  7.032563  ,  2.081117  ,  0.        ,  4.3926105 ,\n",
       "         0.        ,  0.        , 12.355655  ,  1.0077223 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.8280532 ,\n",
       "         0.        ,  0.        ,  7.8808546 ]], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 * -0.06308482587337494 = 0.0\n",
      "0.000, -0.063\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.06475932896137238 = 0.0\n",
      "0.000, 0.065\n",
      "0.000, 0.000\n",
      "\n",
      "3.182011365890503 * 0.06073121726512909 = 0.1996130794286728\n",
      "3.182, 0.061\n",
      "0.193, 0.200\n",
      "\n",
      "0.0 * 0.10466907918453217 = 0.0\n",
      "0.000, 0.105\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.03408030420541763 = 0.0\n",
      "0.000, 0.034\n",
      "0.000, 0.000\n",
      "\n",
      "6.688619136810303 * -0.02090444788336754 = -0.13406743109226227\n",
      "6.689, -0.021\n",
      "-0.140, -0.134\n",
      "\n",
      "6.57066535949707 * 0.04098870977759361 = 0.2542012631893158\n",
      "6.571, 0.041\n",
      "0.269, 0.254\n",
      "\n",
      "0.0 * -0.03825875744223595 = 0.0\n",
      "0.000, -0.038\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.20498262345790863 = 0.0\n",
      "0.000, -0.205\n",
      "-0.000, 0.000\n",
      "\n",
      "2.404444694519043 * 0.04435538128018379 = 0.10525590926408768\n",
      "2.404, 0.044\n",
      "0.107, 0.105\n",
      "\n",
      "3.574993371963501 * 0.02021992765367031 = 0.07150450348854065\n",
      "3.575, 0.020\n",
      "0.072, 0.072\n",
      "\n",
      "0.0 * -0.28701046109199524 = 0.0\n",
      "0.000, -0.287\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.05877947062253952 = 0.0\n",
      "0.000, -0.059\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.10525525361299515 = 0.0\n",
      "0.000, -0.105\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.11573610454797745 = 0.0\n",
      "0.000, -0.116\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.1170048713684082 = 0.0\n",
      "0.000, -0.117\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.14012764394283295 = 0.0\n",
      "0.000, -0.140\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.05500942841172218 = 0.0\n",
      "0.000, 0.055\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.07371222227811813 = 0.0\n",
      "0.000, 0.074\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.046562470495700836 = 0.0\n",
      "0.000, 0.047\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.04630583897233009 = 0.0\n",
      "0.000, 0.046\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.3208003640174866 = 0.0\n",
      "0.000, -0.321\n",
      "-0.000, 0.000\n",
      "\n",
      "3.9750876426696777 * 0.0014683788176625967 = 0.006068998947739601\n",
      "3.975, 0.001\n",
      "0.006, 0.006\n",
      "\n",
      "0.0 * -0.05872073769569397 = 0.0\n",
      "0.000, -0.059\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.0945957750082016 = 0.0\n",
      "0.000, 0.095\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.17947566509246826 = 0.0\n",
      "0.000, 0.179\n",
      "0.000, 0.000\n",
      "\n",
      "4.38274621963501 * -0.34113097190856934 = -1.5227104425430298\n",
      "4.383, -0.341\n",
      "-1.495, -1.523\n",
      "\n",
      "7.232212066650391 * -0.03365295007824898 = -0.24343092739582062\n",
      "7.232, -0.034\n",
      "-0.243, -0.243\n",
      "\n",
      "0.0 * -0.11841452121734619 = 0.0\n",
      "0.000, -0.118\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.1512671858072281 = 0.0\n",
      "0.000, 0.151\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.1273704618215561 = 0.0\n",
      "0.000, -0.127\n",
      "-0.000, 0.000\n",
      "\n",
      "2.671804428100586 * 0.16279336810112 = 0.42518728971481323\n",
      "2.672, 0.163\n",
      "0.435, 0.425\n",
      "\n",
      "0.0 * -0.11522886902093887 = 0.0\n",
      "0.000, -0.115\n",
      "-0.000, 0.000\n",
      "\n",
      "4.2498626708984375 * 0.0028724668081849813 = 0.012466161511838436\n",
      "4.250, 0.003\n",
      "0.012, 0.012\n",
      "\n",
      "0.30033501982688904 * -0.2969716489315033 = -0.09073291718959808\n",
      "0.300, -0.297\n",
      "-0.089, -0.091\n",
      "\n",
      "1.1679474115371704 * -0.1450815051794052 = -0.1738874316215515\n",
      "1.168, -0.145\n",
      "-0.169, -0.174\n",
      "\n",
      "0.0 * -0.0014181931037455797 = 0.0\n",
      "0.000, -0.001\n",
      "-0.000, 0.000\n",
      "\n",
      "1.29261314868927 * 0.10048973560333252 = 0.1226843073964119\n",
      "1.293, 0.100\n",
      "0.130, 0.123\n",
      "\n",
      "0.0 * 0.1292170286178589 = 0.0\n",
      "0.000, 0.129\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.11992790549993515 = 0.0\n",
      "0.000, 0.120\n",
      "0.000, 0.000\n",
      "\n",
      "5.273468494415283 * -0.11759714782238007 = -0.6312107443809509\n",
      "5.273, -0.118\n",
      "-0.620, -0.631\n",
      "\n",
      "0.0 * -0.20351167023181915 = 0.0\n",
      "0.000, -0.204\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.2067321538925171 = 0.0\n",
      "0.000, -0.207\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.11807681620121002 = 0.0\n",
      "0.000, 0.118\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.04050401970744133 = 0.0\n",
      "0.000, 0.041\n",
      "0.000, 0.000\n",
      "\n",
      "9.827601432800293 * -0.11927992105484009 = -1.199428915977478\n",
      "9.828, -0.119\n",
      "-1.172, -1.199\n",
      "\n",
      "0.0 * -0.09743411839008331 = 0.0\n",
      "0.000, -0.097\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.1632920354604721 = 0.0\n",
      "0.000, -0.163\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.1216554045677185 = 0.0\n",
      "0.000, 0.122\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.03388650342822075 = 0.0\n",
      "0.000, 0.034\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.039392467588186264 = 0.0\n",
      "0.000, -0.039\n",
      "-0.000, 0.000\n",
      "\n",
      "9.037511825561523 * -0.0789482519030571 = -0.727680504322052\n",
      "9.038, -0.079\n",
      "-0.713, -0.728\n",
      "\n",
      "0.0 * 0.14086714386940002 = 0.0\n",
      "0.000, 0.141\n",
      "0.000, 0.000\n",
      "\n",
      "7.082968711853027 * 0.11943062394857407 = 0.872066080570221\n",
      "7.083, 0.119\n",
      "0.846, 0.872\n",
      "\n",
      "0.0 * -0.22037136554718018 = 0.0\n",
      "0.000, -0.220\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.09775752574205399 = 0.0\n",
      "0.000, -0.098\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.030066264793276787 = 0.0\n",
      "0.000, 0.030\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.016858400776982307 = 0.0\n",
      "0.000, -0.017\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.039377570152282715 = 0.0\n",
      "0.000, 0.039\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.35895052552223206 = 0.0\n",
      "0.000, -0.359\n",
      "-0.000, 0.000\n",
      "\n",
      "0.6631492376327515 * 0.151442289352417 = 0.10002104938030243\n",
      "0.663, 0.151\n",
      "0.100, 0.100\n",
      "\n",
      "0.0 * -0.014906900934875011 = 0.0\n",
      "0.000, -0.015\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.00016182483523152769 = 0.0\n",
      "0.000, 0.000\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.019313931465148926 = 0.0\n",
      "0.000, 0.019\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.05411764606833458 = 0.0\n",
      "0.000, -0.054\n",
      "-0.000, 0.000\n",
      "\n",
      "0.31634068489074707 * 0.123218834400177 = 0.040605127811431885\n",
      "0.316, 0.123\n",
      "0.039, 0.041\n",
      "\n",
      "0.0 * -0.19911056756973267 = 0.0\n",
      "0.000, -0.199\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.14216890931129456 = 0.0\n",
      "0.000, -0.142\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.044621024280786514 = 0.0\n",
      "0.000, 0.045\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.20383182168006897 = 0.0\n",
      "0.000, -0.204\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.2638700604438782 = 0.0\n",
      "0.000, -0.264\n",
      "-0.000, 0.000\n",
      "\n",
      "8.314438819885254 * -0.00941519532352686 = -0.08168436586856842\n",
      "8.314, -0.009\n",
      "-0.078, -0.082\n",
      "\n",
      "2.218886137008667 * -0.4238462448120117 = -0.9336640238761902\n",
      "2.219, -0.424\n",
      "-0.940, -0.934\n",
      "\n",
      "0.8922330141067505 * -0.04884515330195427 = -0.044062841683626175\n",
      "0.892, -0.049\n",
      "-0.044, -0.044\n",
      "\n",
      "0.0 * -0.06462346762418747 = 0.0\n",
      "0.000, -0.065\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.0318794846534729 = 0.0\n",
      "0.000, -0.032\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.04737556353211403 = 0.0\n",
      "0.000, 0.047\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.09301291406154633 = 0.0\n",
      "0.000, 0.093\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.07903172075748444 = 0.0\n",
      "0.000, -0.079\n",
      "-0.000, 0.000\n",
      "\n",
      "0.8175011873245239 * -0.13604561984539032 = -0.11161670833826065\n",
      "0.818, -0.136\n",
      "-0.111, -0.112\n",
      "\n",
      "0.0 * -0.2663809061050415 = 0.0\n",
      "0.000, -0.266\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.07282588630914688 = 0.0\n",
      "0.000, 0.073\n",
      "0.000, 0.000\n",
      "\n",
      "2.8819642066955566 * 0.10303833335638046 = 0.28802385926246643\n",
      "2.882, 0.103\n",
      "0.297, 0.288\n",
      "\n",
      "2.4174857139587402 * -0.114358089864254 = -0.2752430737018585\n",
      "2.417, -0.114\n",
      "-0.276, -0.275\n",
      "\n",
      "9.465484619140625 * 0.05397471413016319 = 0.4932191073894501\n",
      "9.465, 0.054\n",
      "0.511, 0.493\n",
      "\n",
      "7.8511505126953125 * -0.20452415943145752 = -1.6614809036254883\n",
      "7.851, -0.205\n",
      "-1.606, -1.661\n",
      "\n",
      "0.0 * 0.03845495730638504 = 0.0\n",
      "0.000, 0.038\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.04659215360879898 = 0.0\n",
      "0.000, -0.047\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.08627378195524216 = 0.0\n",
      "0.000, -0.086\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.4743741750717163 = 0.0\n",
      "0.000, -0.474\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.28316882252693176 = 0.0\n",
      "0.000, -0.283\n",
      "-0.000, 0.000\n",
      "\n",
      "4.834473133087158 * 0.12975788116455078 = 0.6545906662940979\n",
      "4.834, 0.130\n",
      "0.627, 0.655\n",
      "\n",
      "11.010446548461914 * -0.0141247333958745 = -0.1558464616537094\n",
      "11.010, -0.014\n",
      "-0.156, -0.156\n",
      "\n",
      "0.0 * -0.04679657146334648 = 0.0\n",
      "0.000, -0.047\n",
      "-0.000, 0.000\n",
      "\n",
      "0.5549504160881042 * -0.1643649935722351 = -0.09295754879713058\n",
      "0.555, -0.164\n",
      "-0.091, -0.093\n",
      "\n",
      "4.236378192901611 * 0.14192311465740204 = 0.6284897327423096\n",
      "4.236, 0.142\n",
      "0.601, 0.628\n",
      "\n",
      "0.0 * 0.1360066831111908 = 0.0\n",
      "0.000, 0.136\n",
      "0.000, 0.000\n",
      "\n",
      "2.334083080291748 * -0.02280261367559433 = -0.05277840048074722\n",
      "2.334, -0.023\n",
      "-0.053, -0.053\n",
      "\n",
      "0.0 * -0.05957978591322899 = 0.0\n",
      "0.000, -0.060\n",
      "-0.000, 0.000\n",
      "\n",
      "11.079065322875977 * 0.12746478617191315 = 1.4671014547348022\n",
      "11.079, 0.127\n",
      "1.412, 1.467\n",
      "\n",
      "0.0 * -0.2783943712711334 = 0.0\n",
      "0.000, -0.278\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * -0.263845831155777 = 0.0\n",
      "0.000, -0.264\n",
      "-0.000, 0.000\n",
      "\n",
      "3.9959394931793213 * -0.2257295846939087 = -0.9331532120704651\n",
      "3.996, -0.226\n",
      "-0.902, -0.933\n",
      "\n",
      "0.0 * 0.13626030087471008 = 0.0\n",
      "0.000, 0.136\n",
      "0.000, 0.000\n",
      "\n",
      "0.9798336625099182 * -0.2155054807662964 = -0.21827639639377594\n",
      "0.980, -0.216\n",
      "-0.211, -0.218\n",
      "\n",
      "7.002312183380127 * 0.15726439654827118 = 1.0711932182312012\n",
      "7.002, 0.157\n",
      "1.101, 1.071\n",
      "\n",
      "0.0 * -0.22908782958984375 = 0.0\n",
      "0.000, -0.229\n",
      "-0.000, 0.000\n",
      "\n",
      "1.5550460815429688 * -0.0790771022439003 = -0.11767373234033585\n",
      "1.555, -0.079\n",
      "-0.123, -0.118\n",
      "\n",
      "0.0 * -0.10032489895820618 = 0.0\n",
      "0.000, -0.100\n",
      "-0.000, 0.000\n",
      "\n",
      "0.6143631339073181 * 0.08721750974655151 = 0.05270957574248314\n",
      "0.614, 0.087\n",
      "0.054, 0.053\n",
      "\n",
      "0.0 * -0.03236164525151253 = 0.0\n",
      "0.000, -0.032\n",
      "-0.000, 0.000\n",
      "\n",
      "7.032563209533691 * -0.35415294766426086 = -2.4745051860809326\n",
      "7.033, -0.354\n",
      "-2.491, -2.475\n",
      "\n",
      "2.0811169147491455 * 0.11047862470149994 = 0.23383955657482147\n",
      "2.081, 0.110\n",
      "0.230, 0.234\n",
      "\n",
      "0.0 * 0.14510954916477203 = 0.0\n",
      "0.000, 0.145\n",
      "0.000, 0.000\n",
      "\n",
      "4.392610549926758 * 0.038537316024303436 = 0.17423084378242493\n",
      "4.393, 0.039\n",
      "0.169, 0.174\n",
      "\n",
      "0.0 * 0.07889293879270554 = 0.0\n",
      "0.000, 0.079\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.12466131150722504 = 0.0\n",
      "0.000, 0.125\n",
      "0.000, 0.000\n",
      "\n",
      "12.3556547164917 * -0.0053521874360740185 = -0.06178490072488785\n",
      "12.356, -0.005\n",
      "-0.066, -0.062\n",
      "\n",
      "1.00772225856781 * -0.1679038107395172 = -0.17668159306049347\n",
      "1.008, -0.168\n",
      "-0.169, -0.177\n",
      "\n",
      "0.0 * 0.13672581315040588 = 0.0\n",
      "0.000, 0.137\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.19655142724514008 = 0.0\n",
      "0.000, 0.197\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.04905140399932861 = 0.0\n",
      "0.000, 0.049\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * 0.11708090454339981 = 0.0\n",
      "0.000, 0.117\n",
      "0.000, 0.000\n",
      "\n",
      "0.0 * -0.18916283547878265 = 0.0\n",
      "0.000, -0.189\n",
      "-0.000, 0.000\n",
      "\n",
      "2.8280532360076904 * -0.2506788969039917 = -0.739621102809906\n",
      "2.828, -0.251\n",
      "-0.709, -0.740\n",
      "\n",
      "0.0 * -0.04226519167423248 = 0.0\n",
      "0.000, -0.042\n",
      "-0.000, 0.000\n",
      "\n",
      "0.0 * 0.04531633108854294 = 0.0\n",
      "0.000, 0.045\n",
      "0.000, 0.000\n",
      "\n",
      "7.880854606628418 * 0.036574456840753555 = 0.30077406764030457\n",
      "7.881, 0.037\n",
      "0.288, 0.301\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(-5.1689677), -5.280333912931383)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h20 = 0\n",
    "lh20 = 0\n",
    "for a, w in zip(la1[0], fc2_weight[0]):\n",
    "    product = a * w\n",
    "    lproduct = run_lmul(a, w, Float32)\n",
    "    print(f\"{a:.3f}, {w:.3f}\")\n",
    "    print(f\"{product:.3f}, {lproduct:.3f}\\n\")\n",
    "    h20 += product\n",
    "    lh20 += lproduct\n",
    "h20, lh20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.304992480606651e+38"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmul(0, 0.47, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01075647, -0.14156555,  0.01885387, -0.1005828 ,  0.08457243,\n",
       "       -0.13470922, -0.09962139, -0.13597849,  0.15926038,  0.0367002 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.2412014, -12.867119 ,  -1.4991008,   1.0043821, -17.543488 ,\n",
       "        -8.378266 , -18.727932 ,  13.432735 ,  -7.5067835,  -3.5312648],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5730446846132548e+39,\n",
       " 7.731816031081458e+38,\n",
       " 1.3357269073370908e+39,\n",
       " 1.9189771933885903e+39,\n",
       " 1.889127489728345e+39,\n",
       " nan,\n",
       " 1.615604004539853e+39,\n",
       " 2.1378885634611277e+39,\n",
       " 2.5987523737478726e+39,\n",
       " 1.0207589458047743e+39]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = []\n",
    "for col in range(fc2_weight.shape[0]):\n",
    "    output = 0\n",
    "    for row in range(fc2_weight.shape[1]):\n",
    "        output += lmul(la1[0, row], fc2_weight.T[row, col], torch.float32)\n",
    "    final_output.append(output)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  3.1820114 ,  0.        ,  0.        ,\n",
       "         6.688619  ,  6.5706654 ,  0.        ,  0.        ,  2.4044447 ,\n",
       "         3.5749934 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.9750876 ,  0.        ,  0.        ,\n",
       "         0.        ,  4.382746  ,  7.232212  ,  0.        ,  0.        ,\n",
       "         0.        ,  2.6718044 ,  0.        ,  4.2498627 ,  0.30033502,\n",
       "         1.1679474 ,  0.        ,  1.2926131 ,  0.        ,  0.        ,\n",
       "         5.2734685 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         9.827601  ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  9.037512  ,  0.        ,  7.0829687 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.66314924,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.31634068,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  8.314439  ,  2.2188861 ,  0.892233  ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.8175012 ,\n",
       "         0.        ,  0.        ,  2.8819642 ,  2.4174857 ,  9.465485  ,\n",
       "         7.8511505 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  4.834473  , 11.010447  ,  0.        ,  0.5549504 ,\n",
       "         4.236378  ,  0.        ,  2.334083  ,  0.        , 11.079065  ,\n",
       "         0.        ,  0.        ,  3.9959395 ,  0.        ,  0.97983366,\n",
       "         7.002312  ,  0.        ,  1.5550461 ,  0.        ,  0.61436313,\n",
       "         0.        ,  7.032563  ,  2.081117  ,  0.        ,  4.3926105 ,\n",
       "         0.        ,  0.        , 12.355655  ,  1.0077223 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.8280532 ,\n",
       "         0.        ,  0.        ,  7.8808546 ]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.230445 , -12.7255535,  -1.5179547,   1.104965 , -17.62806  ,\n",
       "        -8.243557 , -18.628311 ,  13.568714 ,  -7.6660438,  -3.567965 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 @ fc2_weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06308483, -0.05687173, -0.0834864 , ...,  0.02465488,\n",
       "         0.04058975,  0.07350461],\n",
       "       [ 0.06475933, -0.1105739 ,  0.06604536, ..., -0.24417032,\n",
       "         0.06145906,  0.00463336],\n",
       "       [ 0.06073122,  0.08990667, -0.29389277, ...,  0.21576153,\n",
       "        -0.17927682, -0.13132583],\n",
       "       ...,\n",
       "       [-0.04226519,  0.2001559 ,  0.01896591, ..., -0.05012242,\n",
       "        -0.33452684,  0.10945006],\n",
       "       [ 0.04531633,  0.07759127, -0.07641789, ...,  0.02831677,\n",
       "         0.01258108, -0.08120748],\n",
       "       [ 0.03657446, -0.04862732, -0.07712971, ...,  0.12084612,\n",
       "         0.11918967,  0.10050501]], shape=(128, 10), dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         config weight_type activation_type        multiplier  avg_loss  accuracy   total_time  batch_size  total_batches  total_samples  samples_per_second  error\n",
      "0      w8a8-8x8      Float8          Float8  float_multiplier       NaN      9.59  3696.457879          16            625          10000            2.705293    NaN\n",
      "1  wb16ab16-8x8        BF16            BF16  float_multiplier  1.510172     97.46  6990.566352          16            625          10000            1.430499    NaN\n",
      "2    w8ab16-8x8      Float8            BF16  float_multiplier  1.516955     97.43  7083.499045          16            625          10000            1.411732    NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.read_csv(\"results/mnist_eval.csv\").to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
