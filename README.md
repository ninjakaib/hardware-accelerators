# UCSD DSC180B Capstone - Hardware Accelerators
Winter Quarter 2025

contributers: Kai Breese, Justin Chou, Katelyn Abille, Lukas Fullner

mentor: Rajesh Gupta

## Contributing

I am going to lock the `main` branch to prevent anyone from pushing to it directly. All changes to main will be made exclusively by merging pull requests from the `dev` branch. Pull requests should generally contain code/notebooks that you feel are READY TO BE GRADED! 

To contribute new code, create a branch from `dev` starting with name "feature/" or "fix/" followed by a descriptive name of what you are working on. Lets try to keep branches concise, commit code frequently, review each others PRs frequently, and avoid making "super-branches" where you work on a bunch of stuff in one branch and let it get messy. Keep branches to ideally a single fix or feature, then merge. It will help keep the code organized and easier to work with. 

>**Tip**: Keep your feature branches up to date with changes in `dev` by rebasing on it often. This will keep the commit history cleaner if your branch is ever behind and as the project grows

Good branch names are things like: "feature/systolic-memory-controller" or "fix/latex-workflow"  
Bad branch names are: "kais-dev-branch" or "feature/pyrtl"

Your branches can be merged into `dev` after 1 review from someone who isn't yourself.  

We should aim to have changes in `dev` ready to merge into `main` before 11:59PM every Sunday so we have concrete progress to report to Rajesh and for the participation. Pull requests to `main` will only be allowed to merge after 3 reviews, let's stay on top of things guys!

## Project structure

- `./hardware_accelerators` will contain all source code (PyRTL, Verilog, etc.)
- `./tests` will contain `pytest` tests that are automatically run as part of a CI pipeline
- `./reports` contains the source LaTeX files for the report and the pdf generated by the github action
- `./notebooks` should hold all jupyter notebook files. The main branch should only have high quality, readable, reproduceable notebooks.

## Q2 Roadmap

### Outline

1. Architectural level planning, requirements analysis, specification design, and division of work

   > - Google TPU ([paper](https://arxiv.org/abs/1704.04760), [blog](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu))
   > - AWS Inf2 [architecture](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inferentia2.html)
   > - NVIDIA H100 [tensor core architecture](https://resources.nvidia.com/en-us-tensor-core)
   > - [Etched Transformer ASIC](https://www.etched.com/announcing-etched)

2. The hardware itself containing the logic for specialized modules/operations

- [PyRTL](https://sites.cs.ucsb.edu/~sherwood/pubs/FPL-17-pyrtl.pdf) is an easy to use hardware description language that allows us to generate and simulate hardware in Python and automatically generate Verilog code as output
- [Amaranth](https://amaranth-lang.org/docs/amaranth/) (previously nMigen) is similar to PyRTL but more mature, maintained, and provides better integration with tooling. I think it is worth considering switching to avoid potential limitations with PyRTL
  > - [example ALU in PyRTL](https://github.com/pllab/pipelined-alu/blob/master/README.md)
  > - [example RISCV CPU in PyRTL](https://github.com/pllab/BD-PyRTL-RV)

3. A set of opcodes/instructions that can interface with simulated memory

- [OWL](https://zsisco.net/papers/control-logic-synthesis.pdf) is a tool from the same lab that made PyRTL that automatically generates control logic for a given datapath ([github](https://github.com/UCSBarchlab/owl))
- [Instruction level abstraction](https://arxiv.org/pdf/1801.01114) allows us to generate PyRTL code with OWL by describing the behavior of the instruction set ([github]())
  > - [Example opcodes](https://github.com/pllab/embedded-class-riscv/blob/master/src/control.py) for a RISCV CPU in PyRTL (different project)

4. A fast, easy to use, and interactive simulation of the hardware that allows us to easily test sending inputs and getting outputs

- [Yosys](https://yosyshq.readthedocs.io/projects/yosys/en/latest/) is a framework for Verilog RTL synthesis.
- [Surfer VSCode extension](https://marketplace.visualstudio.com/items?itemName=surfer-project.surfer) lets you easily view the waveform outputs of simulation tests
- [OSS CAD Suite](https://github.com/YosysHQ/oss-cad-suite-build) is a binary software distribution for a number of open source software used in digital logic design. You will find tools for RTL synthesis, formal hardware verification, place & route, FPGA programming, and testing with support for HDLs like Verilog, Migen, and Amaranth.

5. An assembly language and assembler that compiles to the custom bytecode (Nvidia PTX)

   > - Ben Eater's [assembler](https://github.com/TheTask/8Bit-Assembler) for his [8-bit computer](https://eater.net/8bit) from scratch project

6. A custom programming language/library/SDK that compiles to our assembly lang (think CUDA, ROCm, Apple CoreML)

- [LLVM](https://llvm.org/docs/GettingStarted.html) is a compiler infrastructure that can be used to create a custom language, but this is probably overkill

7. Integrating the library with a machine learning framework or simulate using pure python.
   > We could integrate [ONNX](https://onnx.ai/onnx/intro/concepts.html), the open neural network exchange format which supports most modern models and hardware by extending the [onnxruntime](https://onnxruntime.ai/docs/reference/high-level-design.html) with a new [ExecutionProvider](https://onnxruntime.ai/docs/execution-providers/add-execution-provider.html) that uses our hardware accelerator. This would allow us to run almost any model in simulation. ONNX generates a computational graph of a model and the execution providers route subgraphs to accelerators based on the implemented kernels/operators.

---

### Bonus

1. Create a physical design after verifying simulations

- [OpenLane](https://openlane.readthedocs.io/en/latest/) is an automated RTL to GDSII flow based on several components including OpenROAD and Yosys
- [OpenROAD](https://github.com/The-OpenROAD-Project/OpenROAD) is the leading open-source, foundational application for semiconductor digital design. The OpenROAD flow delivers an Autonomous, No-Human-In-Loop (NHIL) flow, 24 hour turnaround from RTL-GDSII for rapid design exploration and physical design implementation.
- [IIC-OSIC-TOOLS](https://github.com/iic-jku/IIC-OSIC-TOOLS) is an all-in-one Docker image for SKY130/GF180/IHP130-based analog and digital chip design.

2. Include an interactive 3D model of the GDS files in the project

- [Tiny Tapeout](https://tinytapeout.com/) provides a [guide](https://tinytapeout.com/guides/workshop/create-your-gds/) for generating a github pages site with an interactive 3D model of the GDS files in the project
- [GDS2WebGL](https://github.com/s-holst/GDS2WebGL): This tool provides a performant, portable, and approachable way to visualize and browse chip layout data. It does so by translating the geometric shapes found in GDSII stream format into a self-contained HTML file that can be viewed in any modern WebGL-capable web browser.
- [Example](https://mattvenn.github.io/wokwi-verilog-gds-test/viewer/tinytapeout.html) GitHub pages project
