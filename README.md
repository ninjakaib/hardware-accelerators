<div align="center">

# Hardware Accelerators
### UCSD DSC180B Capstone Project | Winter 2025

<!-- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md) -->

</div>

## Team

**Mentor**: Professor Rajesh Gupta

**Contributors**:
- Kai Breese
- Justin Chou
- Katelyn Abille
- Lukas Fullner

---

## Project Overview

This project explores the implementation of the Linear-complexity Multiplication (L-Mul) algorithm for efficient floating-point multiplication on ASICs. As modern AI systems grow in scale and complexity, their computational processes require increasingly large amounts of energy. For example, ChatGPT required an estimated 564 MWh per day as of February 2023, with inference costs significantly outweighing training costs in the long term.

### The L-Mul Algorithm
The core idea of L-Mul is to eliminate the costly mantissa multiplication step in floating-point multiplication by approximating it with a simpler term L(M), where M is the number of mantissa bits. This approximation achieves high precision while significantly reducing computational overhead compared to traditional floating-point multiplication methods.

### Quarter 1 Accomplishments
In Q1, we:
- Implemented the L-Mul algorithm in PyRTL and Verilog
- Developed a 2x2 systolic array for matrix multiplication using L-Mul
- Validated the algorithm's accuracy against standard floating-point multiplication
- Created basic components for handling BFloat16 numbers
- Successfully demonstrated that theoretical simplifications translate to practical hardware designs

### Quarter 2 Goals
Building on our Q1 work, we aim to:
- Focus exclusively on PyRTL implementation for faster development
- Expand and optimize our systolic array implementation
- Build hardware activation units for activation functions commonly used in machine learning
- Benchmark performance against traditional floating-point multiplication
- Run models on simulated hardware

### Why This Matters
By optimizing the fundamental operation of floating-point multiplication, we can significantly reduce the energy consumption and processing time of neural network operations. This has important implications for making AI systems more environmentally sustainable and cost-effective, particularly in the inference phase where computational costs are highest.

For more details about our Q1 work, see our [technical report](reports/main.pdf).

---

## Project structure

- `./hardware_accelerators` will contain all source code (PyRTL, Verilog, etc.)
- `./tests` will contain `pytest` tests that are automatically run as part of a CI pipeline
- `./reports` contains the source LaTeX files for the report and the pdf generated by the github action
- `./notebooks` should hold all jupyter notebook files. The main branch should only have high quality, readable, reproduceable notebooks that can be used to help write the final report.

---

## Contributing

This project follows the [GitHub Flow](https://docs.github.com/en/get-started/quickstart/github-flow) branching strategy. The `main` branch is locked to prevent direct pushes - all changes must be made through pull requests.

### Branch Guidelines

- Create feature branches for specific tasks/features
- Branch names should be descriptive and follow the pattern: `feature/` or `fix/` followed by what you're working on
- Keep branches focused on single features/fixes to maintain clean version control
- Commit code frequently with clear commit messages

Good branch names: `feature/systolic-memory-controller` or `fix/latex-workflow`  
Bad branch names: `kais-dev-branch` or `feature/misc-changes`

### Working with Branches

Create and switch to a new feature branch:
```bash
# Create and checkout new branch
git checkout -b feature/your-feature-name main

# Set upstream to track remote branch
git push -u origin feature/your-feature-name
```

You can also configure git to automatically create and track a new remote branch with:
```bash
git config --global --type bool push.autoSetupRemote true
```
Then simply `git push` to create or update the new branch on GitHub.

Keep your feature branch up to date with main:
```bash
# Fetch latest changes
git fetch origin main

# Rebase your branch on main
git rebase origin/main

# Force push if you've rebased (be careful!)
git push --force-with-lease
```

### Pull Request Process

1. Ensure your code is well-tested and documented
2. Create a pull request against the `main` branch
3. PRs require 2 approvals from other contributors before merging
4. Use the PR description to explain your changes and any important considerations
5. Reviewers should provide constructive feedback and test the changes locally if needed

> **Tip**: Rebase your feature branches on `main` frequently to keep the commit history clean and avoid complex merge conflicts later.

Remember to commit code frequently and keep your branches focused on specific tasks. This helps maintain a clean version history and makes code review easier for everyone.

After a pull request is merged into `main`, your branch will automatically be deleted. You can update up your local repository to reflect these changes with `git fetch --prune`.
